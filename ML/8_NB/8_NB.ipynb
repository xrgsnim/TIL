{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Naive Bayes Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Probability Overview]** - 머신러닝의 학습방법들\n",
    "- Gradient descent based learning\n",
    "- Probability theory based learning - 이번에 배울거\n",
    "- Information theory based learning\n",
    "- Distance similarity based learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability**\n",
    "- 이산형 값\n",
    "> ##### $P(X) = \\frac{count(Event_X)}{count(ALL_{Event})}$\n",
    "\n",
    "- 연속형 값\n",
    "> $P(-\\infty < x < \\infty) = \\int_{-\\infty}^{\\infty} f(x)dx = 1$\n",
    "<br><br><br>\n",
    "\n",
    "**Basic concepts of probability**\n",
    "> $0 \\le P(E) \\le 1$ <br>\n",
    "> $P(S) = \\sum_{i=1}^N P(E_i) = 1$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if all $E_i$ are independent\n",
    "<br><br><br>\n",
    "\n",
    "**Conditional probability**\n",
    "> $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "**[Bayes's Theorem]**\n",
    "- 경험에 의한 확률 업데이트\n",
    "- 주어진 사전 확률에서 특정 사건 발생을 통해 사후 확률로 계속해서 업데이트(ex.개표방송)\n",
    "- 빈도주의 vs 베이즈주의\n",
    "- 객관적 확률은 존재하지 않는다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability updated**\n",
    "- 특정 사건이 일어남에 따라 확률을 업데이트(ex. 퀸찾기 카드게임)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes's theorem**\n",
    "> $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$P(B|A) = \\frac{P(A \\cap B)}{P(A)}$ <br>\n",
    "> $P(A \\cap B) = P(B)P(A|B) = P(A)|P(B|A)$ <br><br>\n",
    "> $P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A)P(B|A)}{P(B)}$\n",
    "- 다시말해서, 사건B가 일어났을 때의 사건A가 일어날 확률은, 사건B가 일어났을때, 사건A가 일어날 확률은 주어져있고, 사건A가 일어났을때 사건B가 일어날 확률을 거기에 곱해주면 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $P(H|D) = \\frac{P(H)P(D|H)}{P(D)}$ <br>\n",
    "(H is Class &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; D is Data)\n",
    "- P(H|D) : 사후확률, 어떤 데이터가 주어졌을 때의 가설이 발생할(발생하지않을) 확률 (H : 가설함수)\n",
    "- P(D) : 전체확률, 데이터가 발생할 확률(Evidence)\n",
    "- P(H) : 사전확률, 예) 1000개중 $y_1$= 700, $y_2$=300개면 P($H_1$)=700/1000\n",
    "- 결국은 주어진 데이터(D) X를 통해 클래스(D) Y를 찾는 작업\n",
    "- 나중에는 P(D)는 고정값이 되고, P(H)는 주어져서 결국 P(D|H), 우도를 찾는게 가장 중요해진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "**[Naive Bayes Classifier]**<br><br>\n",
    "예제로 풀어보기 - Viagra 스팸 필터기\n",
    "- Viagra라는 단어의 유무를 통해 스팸 여부 확인\n",
    "- Viagra 단어가 들어가면 무조건 스팸?\n",
    "- 어느정도 확률로 스팸이라고 해야할까? <br><br>\n",
    "![img](https://ifh.cc/g/3vSPxP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $P(spam | viagra) = \\frac{P(spam)P(viagra|spam)}{P(viagra)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>viagra</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   viagra  spam\n",
       "0       1     1\n",
       "1       0     0\n",
       "2       0     0\n",
       "3       0     0\n",
       "4       0     0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viagra_spam = {'viagra': [1,0,0,0,0,0,0,0,1,1,1,0,0,1,0,0,0,0,0,1],\n",
    "               'spam': [\n",
    "                   1,0,0,0,0,0,1,0,1,0, 0,0,0,0,0,0,0,1,1,1\n",
    "               ]}\n",
    "\n",
    "df = pd.DataFrame(viagra_spam, columns=['viagra', 'spam'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 1],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_data = df.values\n",
    "np_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P(viagra), P(spam), P(V $\\cap$ S), P($N^c \\cap S$) 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_viagra = sum(np_data[:, 0] == 1) / len(np_data)\n",
    "p_spam = sum(np_data[:, 1] == 1) / len(np_data)\n",
    "p_v_cap_s = sum((np_data[:, 0] == 1) & (np_data[:, 1] == 1)) / len(np_data)\n",
    "p_n_v_cap_s = sum((np_data[:, 0] == 0) & (np_data[:, 1]== 1)) / len(np_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P(Spam | Viagra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_spam * (p_v_cap_s / p_spam) / p_viagra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P(Spam | ~Viagra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2142857142857143"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_spam * (p_n_v_cap_s / p_spam) / (1-p_viagra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "**제대로된 스팸 필터기를 만들어보자**\n",
    "- Viagra 단어외에 영향을 주는 단어들은?\n",
    "- 오히려 스팸을 제외해주는 단어는 어떻게 찾을까?\n",
    "- 한번에 여러 단어들을 고려하는 필터기를 만들어보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature의 확장**<br><br>\n",
    "$P(spam | viagra)$ &nbsp;&nbsp;( spam : y &nbsp;&nbsp;&nbsp;&nbsp; viagra : x )<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\Downarrow$<br>\n",
    "$P(spam | viagra, hello, lucky, marketing, \\cdots)$\n",
    "- y는 그대로 1또는 0을 가지지만, x는 $x_1, x_2, x_3 \\cdots $로 확장됨\n",
    "- 변수가 많을 때, 조건부 확률의 변화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Multivariate multiplication rule ]**\n",
    "> $P(Y | X_1 \\cap X_2) = \\frac{P(Y \\cap X_1 \\cap X_2)}{P(X_1 \\cap X_2)}$<br>\n",
    "> $P(Y \\cap X_1 \\cap X_2) = P(Y | X_1, X_2) P(X_1 \\cap X_2)$ <br><br>\n",
    "> $P(X_1, X_2, X_3, \\cdots, X_n)$<br>\n",
    "> $= P(X_1)P(X_2|X_1)P(X_3|X_1, X_2) \\cdots P(X_n|X_1 \\cdots X_{x-1})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problems!**\n",
    "- 변수가 늘어날수록 급격히 계산이 어려워짐\n",
    "- Feature의 차원이 증가하면 Sparse Vector가 생성 $\\rightarrow$ 확률이 0이 되는 값이 늘어남"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **[ Naive Bayes Classifier ]**\n",
    "- 접근방향 : 복잡하게 하지만고 단순(naive)하게 해결하자\n",
    "- 각 변수의 관계가 독립임을 가정\n",
    "- 계산이 용이해지고, 성능이 생각보다 좋음..!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joint Probability**\n",
    "> $P(A \\cap B) = P(A)P(B)$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if A and B are independent<br><br>\n",
    "> $P(Y|X_1 \\cap X_2) = \\frac{P(Y)P(X_1 \\cap X_2 | Y)}{P(X_1 \\cap X_2)}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$= \\frac{P(Y)P(X_1|Y)P(X_2|Y)}{P(X_1)P(X_2)}$\n",
    "- 이전의 Multivariate multiplication rule보다는 더 간결해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes Classifier**\n",
    "> ##### $P(Y|X_1 \\cap X_2) = \\frac{P(Y) P(X_1 \\cap X_2 | Y)}{P(X_1 \\cap X_2)} = \\frac{P(Y)P(X_1|Y)P(X_2|Y)}{P(X_1)P(X_2)}$<br><br>\n",
    "> ##### <span style='background-color:#fff5b1'>$P(Y_c | X_1, \\cdots,X_n) > = \\frac{P(Y_c) \\prod_{i=1}^n P(X_i |Y_c)}{\\prod_{i=1}^n P(X_i)}$</span> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;$Y_c$ : label ($Y_c$ = 0 or 1)\n",
    "- $X_1$부터 $X_n$까지의 Data가 주어졌을 때, P(Y=1)과 P(Y=0)중에 더 큰 값을 채택하여 모든 데이터마다 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Issue!**\n",
    "- 너무 많은 확률값$(0 \\le P \\le 1) \\rightarrow$ 0에 수렴하게 되는문제\n",
    "- 곱하지 말고 더하자 $\\rightarrow$ log\n",
    "<br><br>\n",
    "> $log({P(Y_c)\\prod_{i=1}^n P(X_i|Y_c)}) = log P(Y_c) + \\sum_{i=1}^n log P(X_i |Y_c)$\n",
    "> ##### <span style='background-color:#fff5b1'>$P(Y_c | X_1, \\cdots, X_n) = \\frac{P(Y_c) \\prod_{i=1}^n P(X_i|Y_c)}{\\prod_{i=1}^n P(X_i)}$</span> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Y_c$ : label ($Y_c$ = 0 or 1)\n",
    "- 분모에서 $X_i$는 피쳐이기 때문에 prod의 결과값은 고정되어있어서 중요하지 않다.<br>\n",
    "$\\divideontimes$ Likelihood : 가능도 <br>\n",
    "<br><br>\n",
    "- 확률이 0인 변수들이 존재함 $\\rightarrow$ 전체값 0\n",
    "- 작게나마 확률이 나올 수 있도록변경 $\\rightarrow$ **스무딩**\n",
    "> #### $P(X|Y) = \\frac{count(X \\cap Y) + k}{count(Y) + (k\\;*\\;|number\\;of\\;class|)}$\n",
    "- 분모분자에 k라는 상수값을 임의로 넣어서 값을 좀 더 평탄하게(generalize) 해준다. 다만 count값들에 비해 k가 커버리면 차이가 더 흐려질수도 있으니 주의\n",
    "- 스무딩 기법을 사용하면 확률이 0으로 산출되지도 않고 generalize의 효과도 볼수있다.\n",
    "- binary한 분류문제에서는 Y=1 or 0이 될테니까 number of class=2가 된다\n",
    "> ##### <span style='background-color:#fff5b1'> $log{P(Y_c)\\prod_{i=1}^n P(X_i|Y_c)} = log P(Y_c) + \\sum_{i=1}^n logP(X_i | Y_c)$</span>\n",
    "- 최종식. 우변에서 sum항에 스무딩을 먹여줘서 0인 값이 없도록 한다.\n",
    "<br><br>\n",
    "- NB의 핵심 : 최종 식 도출 + 스무딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### **NB Classifier Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset - German Credit**\n",
    "- 대출 사기인가 아닌가를 예측하는 문제\n",
    "- 데이터를 NB에 맞도록 간단하게 변환\n",
    "- Binary 데이터들로 이루어진 대출 사기 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Load, Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>History</th>\n",
       "      <th>CoApplicant</th>\n",
       "      <th>Accommodation</th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>current</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>paid</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>paid</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>paid</td>\n",
       "      <td>guarantor</td>\n",
       "      <td>rent</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>arrears</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  History CoApplicant Accommodation  Fraud\n",
       "0   1  current        none           own   True\n",
       "1   2     paid        none           own  False\n",
       "2   3     paid        none           own  False\n",
       "3   4     paid   guarantor          rent   True\n",
       "4   5  arrears        none           own  False"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/TeamLab/machine_learning_from_scratch_with_python/master/code/ch11/fraud.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False,  True, False,  True, False, False, False,\n",
       "        True, False,  True,  True, False, False, False, False, False,\n",
       "       False, False])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df[\"ID\"]\n",
    "\n",
    "Y_data = df.pop(\"Fraud\")\n",
    "Y_data = Y_data.as_matrix()\n",
    "Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>History</th>\n",
       "      <th>CoApplicant</th>\n",
       "      <th>Accommodation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>current</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paid</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paid</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paid</td>\n",
       "      <td>guarantor</td>\n",
       "      <td>rent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arrears</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   History CoApplicant Accommodation\n",
       "0  current        none           own\n",
       "1     paid        none           own\n",
       "2     paid        none           own\n",
       "3     paid   guarantor          rent\n",
       "4  arrears        none           own"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**\n",
    "- History_arrears = x1, History_current = x2 ... 이런식으로 onehot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>History_arrears</th>\n",
       "      <th>History_current</th>\n",
       "      <th>History_none</th>\n",
       "      <th>History_paid</th>\n",
       "      <th>CoApplicant_coapplicant</th>\n",
       "      <th>CoApplicant_guarantor</th>\n",
       "      <th>CoApplicant_none</th>\n",
       "      <th>Accommodation_free</th>\n",
       "      <th>Accommodation_own</th>\n",
       "      <th>Accommodation_rent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   History_arrears  History_current  History_none  History_paid  \\\n",
       "0                0                1             0             0   \n",
       "1                0                0             0             1   \n",
       "2                0                0             0             1   \n",
       "3                0                0             0             1   \n",
       "4                1                0             0             0   \n",
       "\n",
       "   CoApplicant_coapplicant  CoApplicant_guarantor  CoApplicant_none  \\\n",
       "0                        0                      0                 1   \n",
       "1                        0                      0                 1   \n",
       "2                        0                      0                 1   \n",
       "3                        0                      1                 0   \n",
       "4                        0                      0                 1   \n",
       "\n",
       "   Accommodation_free  Accommodation_own  Accommodation_rent  \n",
       "0                   0                  1                   0  \n",
       "1                   0                  1                   0  \n",
       "2                   0                  1                   0  \n",
       "3                   0                  0                   1  \n",
       "4                   0                  1                   0  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df = pd.get_dummies(df)\n",
    "x_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- onehot encoding의 결과로 x1부터 x10까지의 feature들이 나왔음\n",
    "- 따라서 x의 벡터들이 주어졌을 때, P(y=1)과 P(y=0)중 더 큰걸 선택할수 있도록 classifier를 만들어줄거임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 1, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = x_df.as_matrix()\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling**\n",
    "##### $P(Y_c | X_1, \\cdots, X_n) = \\frac{P(Y_c) \\prod_{i=1}^n P(X_i|Y_c)}{\\prod_{i=1}^n P(X_i)}$<br>\n",
    "- 위 식을 따르는 Naive Bayes's Classifier를 만든다.\n",
    "- $x_1, \\cdots ,x_n$ 은 $x_1, \\cdots ,x_{10}$ 까지\n",
    "- 분모는 모든 y의 값에 적용되니 무시해도된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3, 0.7)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_Y_True = sum(Y_data == True) / len(Y_data)\n",
    "P_Y_False = 1 - P_Y_True\n",
    "\n",
    "P_Y_True, P_Y_False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Y의 Index : 사기가 있는것과 없던것의 인덱스 구분\n",
    "- np.where() : 괄호안의 조건에 성립하는것의 인덱스값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([ 0,  3,  5,  9, 11, 12], dtype=int64),),\n",
       " (array([ 1,  2,  4,  6,  7,  8, 10, 13, 14, 15, 16, 17, 18, 19],\n",
       "        dtype=int64),))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사기 당한 인덱스들\n",
    "ix_Y_True = np.where(Y_data)\n",
    "# 사기 안당한 인덱스들\n",
    "ix_Y_False = np.where(Y_data == False)\n",
    "\n",
    "ix_Y_True, ix_Y_False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $P(X_i | Y_c) = \\frac{Count(X_i \\cap Y = 1)}{Count(Y=1)} \\cdots$ (Y=0일때도)\n",
    "- (x_data[ix_Y_True].sum(axis=0)) 부분 설명<br><br>\n",
    "![img](https://ifh.cc/g/52Dvnx.png)\n",
    "- 위에서 뽑은 인덱스들의 행만 뽑은 뒤 각 피쳐별로 위에서 아래로(axis=0) 쭉 sum 진행.\n",
    "- 그 결과값에 인덱스의 갯수로 나눠줘서 위의 식 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.16666667, 0.5       , 0.16666667, 0.16666667, 0.        ,\n",
       "        0.16666667, 0.83333333, 0.        , 0.66666667, 0.33333333]),\n",
       " array([0.42857143, 0.28571429, 0.        , 0.28571429, 0.14285714,\n",
       "        0.        , 0.85714286, 0.07142857, 0.78571429, 0.14285714]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_x_y_true = (x_data[ix_Y_True].sum(axis=0)) / sum(Y_data == True) # 분모 : count(Y=1)을 의미\n",
    "p_x_y_false = (x_data[ix_Y_False].sum(axis=0)) / sum(Y_data == False) # 분모 : count(Y=0)을 의미\n",
    "\n",
    "p_x_y_true, p_x_y_false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifier**\n",
    "- 사실 아래 식에서 우변의 각 항에 log를 취해주거나 그게 아니면 저 두 항을 서로 곱해줘야하는데 값의 큰 차이가 없어서 더해주면서 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6333333333333333, 1.7714285714285714)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = [0, 1, 0, 0, 0, 1, 0, 0, 1, 0]\n",
    "\n",
    "p_y_true_test = P_Y_True + p_x_y_true.dot(x_test) # P(Y = 1)\n",
    "p_y_false_test = P_Y_False + p_x_y_false.dot(x_test) # P(Y = 0)\n",
    "\n",
    "p_y_true_test, p_y_false_test\n",
    "# P(Y = 1) < P(Y = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_y_true_test < p_y_false_test\n",
    "# 따라서 x_test 데이터에 대해서는 대출사기가 아니라고 하는것이 맞다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multinomial Naive Bayes**\n",
    "- X값이 Binary가 아니라 1이상의 값을 가지는 문제\n",
    "- 일반적으로 Text 문제를 분류할 때 많이 쓰임\n",
    "- 단어의 존재 유무가 아닌 단어의 **출현횟수** Feature로\n",
    "- 구성된 벡터가 sparse할 때 많이 쓰임 ([0 0 0 1 0 0 7 0 0 1 0 0]같은)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Text를 문자로 표현하는 방법 ]**<br><br>\n",
    "**문자를 Vector로 - One hot Encoding**\n",
    "- 하나의 단어를 Vector의 Index로 인식, 단어 존재시 1 없으면 0<br><br>\n",
    "![img](https://ifh.cc/g/bSmZjf.png)\n",
    "- 어떤 단어 자체를 인덱스를 먹여서 존재여부로 표현\n",
    "<br><br>\n",
    "- 하지만 한 문장에서 한번에 여러 단어가 나올때는..?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Bag of words ]**\n",
    "- 단어별(One-hot)로 인덱스를 부여해서 한 문장(또는 문서)의 단어의 개수를 Vector로 표현\n",
    "- 순서의 정보는 표현할 수 없지만 성능이 강력해서 주로 이용됨<br><br>\n",
    "![img](https://ifh.cc/g/9Dwk92.png)<br>\n",
    "(문장에서 the가 두번나오니 the의 초록색 박스의 숫자는 2가 되어야함 )<br>\n",
    "\n",
    "- Bag of words 예시<br>\n",
    "![img](https://ifh.cc/g/gqFPVY.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Multinomial Naive Bayes ]**\n",
    "- 식 계산하는 방식이 다름<br>\n",
    "\n",
    "> $\\prod_{i=1}^n P(X_i | Y_c)$에서의 $P(X_i | Y_c)$<br>\n",
    "> ### $ \\Rightarrow P(X_i | Y_c) = \\frac{\\sum t f(x_i, d \\in Y_c) + \\alpha}{\\sum N_{d \\in Y_c} + \\alpha \\centerdot V}$\n",
    "\n",
    "- $\\sum t f(x_i, d \\in Y_c)$ : 특정 클래스에서 특정 x의 갯수들의 합\n",
    "- $\\sum N_{d \\in y_c}$ : 클래스에 속한 모든 단어의 갯수의 합\n",
    "- $x_i$ : $Y_c$가 특정상황일때, feature의 출현횟수\n",
    "- V : feature의 갯수(단어의 갯수)\n",
    "- $\\alpha$ : 스무딩 파라메터<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 풀어보기<br><br>\n",
    "![img](https://ifh.cc/g/zJMXB9.png)<br><br>\n",
    "> ##### $P(X_i | Y_c) = \\frac{\\sum tf(x_i, d \\in Y_c) + \\alpha}{\\sum N_{d \\in Y_c} + \\alpha \\centerdot V} \\cdots\\cdots \\mathit{1}$\n",
    "> ##### $P(Y_c | X_1, \\cdots, X_n) = \\frac{P(Y_c) \\prod_{i=1}^n P(X_i|Y_c)}{\\prod_{i=1}^n P(X_i)} \\cdots\\cdots \\mathit{2}$<br>\n",
    "**[ Train ]**<br>\n",
    "- P(c) = 3/4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;P(j) = 1/4<br>\n",
    "$\\alpha=2$로 가정\n",
    "\n",
    "- $X_1$ = Chinese<br>$X_2$ = Beijing<br>$X_3$ = Shanghai<br>$X_4$ = Macao<br>$X_5$ = Tokyo<br>$X_6$ = Japan\n",
    "\n",
    "식 1의 $P(X_i|Y_c)$구하기 위해서 $P(X_i | c)$과$P(X_i | j)$를 구할것임<br><br>\n",
    "**1**. $P(X_1 | c)$ : class가 china일 때, $X_1$이 들어있는 확률 \n",
    "- V = 6\n",
    "- $N_{d \\in c}$ = 8\n",
    "- $\\sum tf(x_1, d \\in c)$ = china클래스 안에서 Chinese의 갯수 = 5\n",
    "- 따라서, $P(X_1 | c) = \\frac{5 + 2}{8 + 12}$\n",
    "\n",
    "**2**. $P(X_i | j)$ : class가 japan일 때, $X_1$이 들어있는 확률\n",
    "- V = 6\n",
    "- $N_{d \\in j}$ = 3\n",
    "- $\\sum tf(x_1, d \\in c)$ = japan클래스 안에서 Chinese의 갯수 = 1\n",
    "- 따라서, $P(X_1 | j) = \\frac{1 + 2}{3 + 12}$\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\vdots$<br>\n",
    "- $X_6$까지 모두 구한다음 $\\prod_{i=1}^n P(X_i| c)$와 $\\prod_{i=1}^n P(X_i| j)$를 해서 다 곱해준 뒤 식 2를 구할 때 활용한다.<br><br><br>\n",
    "\n",
    "**[ Test ]**\n",
    "- 식 2를 활용하여 주어진 데이터가 class c에 속할지 j에 속할지 구분해야된다.\n",
    "- Test데이터에 Chinese, Tokyo, Japan이라는 단어가 있으니 $X_1,\\ X_5,\\ X_6$이 있다고 본다.\n",
    "- 따라서 최종적으로 식2를 활용하여,\n",
    "> #### $P(c|X_1, \\cdots, X_n) = \\frac{P(c) * P(X_1|c)* P(X_5|c)* P(X_6|c)}{\\prod_{i=1}^n P(X_i)}$\n",
    "> #### $P(j|X_1, \\cdots, X_n) = \\frac{P(j) * P(X_1|j)* P(X_5|j)* P(X_6|j)}{\\prod_{i=1}^n P(X_i)}$\n",
    "- 위 두 식의 결과 중 더 큰값이 나오는 쪽으로 분류하면 된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "#### **[ Gaussian NB ]**\n",
    "- Category데이터가 아닌 Continuous한 경우에 적용되는 NB\n",
    "- Continuous 데이터의 적용을 위해 y의 분포를 **정규분포(gaussian)으로 가정**함\n",
    "- 확률밀도 함수 상의 해당 값 x가 나올 확률로 NB를 구현함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### $P(x_i | Y_c) = \\frac{1}{\\sqrt{2\\pi \\sigma^2_{Y_c}}}exp(- \\frac{(x_i - \\mu Y_c)^2}{2\\sigma^2_{Y_c}})$\n",
    "- 특정 $Y_c$에 대한 $X_i$의 평균 표준편차를 $\\mu$, $\\sigma$에 대입\n",
    "- 예) Y=1일때, $X_1$이라는 피쳐의 $\\mu$값과 $\\sigma$값을 위 식에 대응시켜줌\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "**예제 풀어보기**\n",
    "<br><br>\n",
    "- 키, 몸무게, 발 사이즈($X_1, X_2, X_3$)를 보고 남자인지 여자인지 예측하는 문제<br>\n",
    "![img](https://ifh.cc/g/xHX3v9.png)<br><br>\n",
    "- 남자or 여자일 경우(Y=1 or 0)의 각 피쳐들($X_i$)의 평균, 분산값들\n",
    "- 아래 값들을 위의 식에 그냥 대입해주면 된다.\n",
    "![img](https://ifh.cc/g/tsQKla.png)\n",
    "<br><br><br>\n",
    "- 그 후에 test데이터가 등장하면 예측해준다.<br>\n",
    "![img](https://ifh.cc/g/w1YYCF.png)\n",
    "\n",
    "- 위의 식 참고해서 확률이 더 큰 값 구해준다.\n",
    "- 여기선 $\\alpha$는 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Naive Bayes with Sklearn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ CountVectorizer in Scikit-learn ]**\n",
    "- 문서에서 Bag of Words Vector를 뽑아주는 class\n",
    "- 단어를 어떻게 뽑을지 정해주는 기능\n",
    "- 다른 전처리 모듈처럼 생성 $\\rightarrow$ 적용의 과정을 거침\n",
    "- A, the 같은 조사들은 **stop word**로써 CountVectorizer내에서 알아서 제거해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 출력값의 앞에서부터 인덱스를 부여받았다.($X_1,X_2,X_3, \\cdots$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['but', 'clean', 'close', 'election', 'forgettable', 'game', 'great', 'it']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_example_text = [\"Sports\", \"Not sports\",\"Sports\",\"Sports\",\"Not sports\"]\n",
    "y_example = [1 if c==\"Sports\" else 0 for c in y_example_text ]\n",
    "text_example = [\"A great game game\", \"The The election was over\",\n",
    "                \"Very clean game match\",\n",
    "                \"A clean but forgettable game game\",\"It was a close election\", ]\n",
    "\n",
    "countvect_example = CountVectorizer()\n",
    "X_example = countvect_example.fit_transform(text_example)\n",
    "countvect_example.get_feature_names()[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 출력값의 첫번째 줄은 A great game game에서의 해당 인덱스가 몇개의 단어를 가지고 있는지 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n",
       "       [1, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_example.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ NB classifier family in scikit-learn ]**\n",
    "- Scikit-learn에서 제공하는 NB classifier\n",
    "- Bernoulli Naive Bayes\n",
    "- Multinomial Naive Bayes\n",
    "- Gausiian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Bernoulli Naive Bayes ]**<br><br>\n",
    "**Parameters**<br>\n",
    "- alpha : 스무딩 파라미터\n",
    "- binarize : threshold, 단어등장 갯수를 세는 경우에 몇개 이상(이하)을 1(0)이라 표현할것인지\n",
    "- fit_prior : 학습으로 class를 나눌것이냐(중요x)\n",
    "- class_prior : 미리 class를 지정해서 나눌것이냐(중요x)<br>\n",
    "\n",
    "**Attributes**<br>\n",
    "- class_log_prior : $log$처리된 $P(Y_c)$출력\n",
    "- feature_log_prob : $log$처리된 Likelihood, $logP(X_i|Y)$\n",
    "- class_count_ : class 몇개인지\n",
    "- feature_count_ : feature 몇개인지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.91629073, -0.51082562])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "clf = BernoulliNB(binarize=0) # 1개만 출연해도 1로\n",
    "clf.fit(X_example, y_example)\n",
    "# Y=1일때 값, Y=0일 때 값\n",
    "clf.class_log_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3.])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.38629436, -1.38629436, -0.69314718, -0.28768207, -1.38629436,\n",
       "        -1.38629436, -1.38629436, -0.69314718, -1.38629436, -0.69314718,\n",
       "        -0.69314718, -1.38629436, -0.28768207],\n",
       "       [-0.91629073, -0.51082562, -1.60943791, -1.60943791, -0.91629073,\n",
       "        -0.22314355, -0.91629073, -1.60943791, -0.91629073, -1.60943791,\n",
       "        -1.60943791, -0.91629073, -1.60943791]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_log_prob_\n",
    "# [Y=0일때 , 각 피쳐들의 logP가 얼마인지], [Y=1일 때 ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 2., 0., 0., 0., 1., 0., 1., 1., 0., 2.],\n",
       "       [1., 2., 0., 0., 1., 3., 1., 0., 1., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_count_\n",
    "# [Y=0일때, 각 피쳐들이 몇번씩 등장했는지], [Y=1일 때]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Multinomial Naive Bayes ]**<br><br>\n",
    "**Parameters**<br>\n",
    "- alpha : 스무딩 파라미터\n",
    "- fit_prior : 학습으로 class를 나눌것이냐(중요x)\n",
    "- class_prior : 미리 class를 지정해서 나눌것이냐(중요x)<br>\n",
    "\n",
    "\n",
    "**Attributes**<br>\n",
    "- class_log_prior_ : $log$처리된 $P(Y_c)$출력\n",
    "- intercept_ : LR에서의 절편, class_log_prior와 대동소이\n",
    "- feature_log_prob : $log$처리된 Likelihood, $logP(X_i|Y)$\n",
    "- coef_ : feature_log_prob와 대동소이\n",
    "- class_count_ : class 몇개인지\n",
    "- feature_count_ : feature 몇개인지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_example, y_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.91629073 -0.51082562]\n",
      "[[-3.09104245 -3.09104245 -2.39789527 -1.99243016 -3.09104245 -3.09104245\n",
      "  -3.09104245 -2.39789527 -3.09104245 -2.39789527 -1.99243016 -3.09104245\n",
      "  -1.99243016]\n",
      " [-2.52572864 -2.12026354 -3.21887582 -3.21887582 -2.52572864 -1.42711636\n",
      "  -2.52572864 -3.21887582 -2.52572864 -3.21887582 -3.21887582 -2.52572864\n",
      "  -3.21887582]]\n",
      "[2. 3.]\n",
      "[[0. 0. 1. 2. 0. 0. 0. 1. 0. 1. 2. 0. 2.]\n",
      " [1. 2. 0. 0. 1. 5. 1. 0. 1. 0. 0. 1. 0.]]\n",
      "[[-2.52572864 -2.12026354 -3.21887582 -3.21887582 -2.52572864 -1.42711636\n",
      "  -2.52572864 -3.21887582 -2.52572864 -3.21887582 -3.21887582 -2.52572864\n",
      "  -3.21887582]]\n",
      "[-0.51082562]\n"
     ]
    }
   ],
   "source": [
    "print(clf.class_log_prior_)\n",
    "print(clf.feature_log_prob_)\n",
    "print(clf.class_count_)\n",
    "print(clf.feature_count_)\n",
    "\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Gaussian Naive Bayes ]**<br><br>\n",
    "**Parameters**<br>\n",
    "- priors : 학습해서 분류할거냐 안그럴거냐\n",
    "\n",
    "\n",
    "**Attributes**<br>\n",
    "- class_prior_ : $P(Y_c)$출력\n",
    "- class_count_ : class 몇개인지\n",
    "- theta_ : $\\mu$\n",
    "- sigma_ : $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x13 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 18 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "# GaussianNB같은 경우는 sparse한 데이터 형태를 싫어해서 numpy로 바꿔주는 toarray를 적용시켜준다.\n",
    "clf.fit(X_example.toarray(), y_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3.]\n",
      "[0.4 0.6]\n",
      "[[0.         0.         0.5        1.         0.         0.\n",
      "  0.         0.5        0.         0.5        1.         0.\n",
      "  1.        ]\n",
      " [0.33333333 0.66666667 0.         0.         0.33333333 1.66666667\n",
      "  0.33333333 0.         0.33333333 0.         0.         0.33333333\n",
      "  0.        ]]\n",
      "[[8.00000000e-10 8.00000000e-10 2.50000001e-01 8.00000000e-10\n",
      "  8.00000000e-10 8.00000000e-10 8.00000000e-10 2.50000001e-01\n",
      "  8.00000000e-10 2.50000001e-01 1.00000000e+00 8.00000000e-10\n",
      "  8.00000000e-10]\n",
      " [2.22222223e-01 2.22222223e-01 8.00000000e-10 8.00000000e-10\n",
      "  2.22222223e-01 2.22222223e-01 2.22222223e-01 8.00000000e-10\n",
      "  2.22222223e-01 8.00000000e-10 8.00000000e-10 2.22222223e-01\n",
      "  8.00000000e-10]]\n"
     ]
    }
   ],
   "source": [
    "print(clf.class_count_)\n",
    "print(clf.class_prior_)\n",
    "print(clf.theta_)\n",
    "print(clf.sigma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### **News group classification**\n",
    "- 앞서 배운 내용들을 text데이터를 분류하는 문제에 적용해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 20 newsgroups Dataset ]**\n",
    "- 대표적인 Text분류 Toy dataset\n",
    "- 20개의 뉴스 텍스트 데이터를 분류하라 !\n",
    "- Multiclass classification의 대표적 문제\n",
    "- 약 20,000개의 news document 존재<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Process for text classification ]**<br><br>\n",
    "![img](https://ifh.cc/g/Zczvxy.png)\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 텍스트 전처리 ]**<br><br>\n",
    "**Data cleansing**\n",
    "- 어떤 Text를 남길것인가?\n",
    "- 특수 문자는 남겨둘 것인가? ('Good' vs 'Good?')\n",
    "- 숫자는 어떻게 할것인가? ('4 seasons' vs 'seasons')\n",
    "- 특수문자를 제거한다면 공백은? (I'm $\\rightarrow$ Im or I m)\n",
    "- 이메일, ip주소 등 특수 문자들의 처리는?<br><br><br>\n",
    "- **텍스트 전처리 과정**<br><br>\n",
    "![img](https://ifh.cc/g/LvvASt.png)\n",
    "- **Tokenizer** : 단어 자르기\n",
    "- **Stop Word Filtering** : 해당 문장을 특성할 수 있도록 너무 흔한 단어는 없앰\n",
    "- **Steaming** : 어간(활용어가 활용할 때에 변하지 않는 부분)만 남김<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Vectorize ]**<br><br>\n",
    "**ngrams** : 한 번에 몇개의 단어를 묶을것인가?<br><br>\n",
    "![img](https://ifh.cc/g/cPLS3G.png)\n",
    "- N = 1 : tokenize한 것 처럼 한 단어씩 분리\n",
    "- N = 2 : 두단어씩 묶어서 분리\n",
    "- 분리된 단어는 하나의 feature가 된다.<br><br><br>\n",
    "\n",
    "**TF-IDF**(Term Frequency - Inverse Document Frequency)<br>\n",
    " : 전체 문서에서 많이 나오는 단어는 중요도를 낮추고, 특정 문서에서만 많이 나오면 중요도를 올림\n",
    "> $w_{i, j} = tf_{i,j} \\times log(\\frac{N}{df_i})$\n",
    "> - i : 단어의 index\n",
    "> - j : 문서의 index\n",
    "> - $tf_{i,j} : Term frequency : j문서에 i단어가 존재한 횟수$\n",
    "> - N : 전체 문서\n",
    "> - $df_i$ : i단어가 있는 문서의 갯수\n",
    "\n",
    "- 위의 식대로 한 단어마다 TF-IDF를 먹임<br><br>\n",
    "<예시><br>\n",
    "![img](https://ifh.cc/g/Tv6qNz.png)<br>\n",
    "- 단어가 4개, 문서가 3개 있는 경우\n",
    "- car를 예로들면, $27 \\times log(\\frac{3}{3})$이기 때문에\n",
    "- x = 1 일때 y = 0이 도출되어서 전체 문서의 입장에서 car는 중요한 단어가 아니라고 판단<br>\n",
    "- 나머지는 아래 그래프처럼 도출되어 y값이 클수록 중요도가 높다고 판단\n",
    "![img](https://ifh.cc/g/SGXDGs.png)<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading**\n",
    "- Scikit-learn 내부 모듈을 활용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR', 'description'])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "news.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 하키에 관한 내용\n",
    "print(news.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# news.target_names에서 10번째에 오는 데이터로 진행\n",
    "news.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 하키에 관한 내용\n",
    "news.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "- 18846개의 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'News' : news.data, 'Target' : news.target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cm...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: mblawson@midway.ecn.uoknor.edu (Matthew ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: Alexander Samuel McDiarmid &lt;am2o+@andrew...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News  Target\n",
       "0  From: Mamatha Devineni Ratnam <mr47+@andrew.cm...      10\n",
       "1  From: mblawson@midway.ecn.uoknor.edu (Matthew ...       3\n",
       "2  From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...      17\n",
       "3  From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...       3\n",
       "4  From: Alexander Samuel McDiarmid <am2o+@andrew...       4"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cm...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: mblawson@midway.ecn.uoknor.edu (Matthew ...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: Alexander Samuel McDiarmid &lt;am2o+@andrew...</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News                    Target\n",
       "0  From: Mamatha Devineni Ratnam <mr47+@andrew.cm...          rec.sport.hockey\n",
       "1  From: mblawson@midway.ecn.uoknor.edu (Matthew ...  comp.sys.ibm.pc.hardware\n",
       "2  From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...     talk.politics.mideast\n",
       "3  From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...  comp.sys.ibm.pc.hardware\n",
       "4  From: Alexander Samuel McDiarmid <am2o+@andrew...     comp.sys.mac.hardware"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target 데이터 -> 문자 라벨링(뉴스마다 어떤 뉴스인지 보기 편하도록 만들기 위해서)\n",
    "def word_labeling(lst, df):\n",
    "    for idx, name in enumerate(lst):\n",
    "        target_data = df['Target']\n",
    "        for idx_, num_label in enumerate(target_data):\n",
    "            if num_label == idx:\n",
    "                df.loc[idx_, 'Target'] = name\n",
    "    return df\n",
    "news_df = word_labeling(news['target_names'], news_df)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data cleansing**\n",
    "- 이메일 제거\n",
    "- 불필요 숫자 제거\n",
    "- 문자 아닌 특수문자 제거\n",
    "- 단어 사이 공백 제거 : 띄어쓰기별로 split해주고 join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleansing(df):\n",
    "    delete_email = re.sub(r'\\b[\\w\\+]+@[\\w]+.[\\w]+.[\\w]+.[\\w]+\\b', ' ', df)\n",
    "    delete_number = re.sub(r'\\b|\\d+|\\b', ' ',delete_email)\n",
    "    delete_non_word = re.sub(r'\\b[\\W]+\\b', ' ', delete_number)\n",
    "    cleaning_result = ' '.join(delete_non_word.split())\n",
    "    return cleaning_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From Mamatha Devineni Ratnam Subject Pens fans...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From edu Matthew B Lawson Subject Which high p...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From hilmi Hilmi Eren Subject Re ARMENIA SAYS ...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From Guy Dawson Subject Re IDE vs SCSI DMA and...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From Alexander Samuel McDiarmid Subject driver...</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News                    Target\n",
       "0  From Mamatha Devineni Ratnam Subject Pens fans...          rec.sport.hockey\n",
       "1  From edu Matthew B Lawson Subject Which high p...  comp.sys.ibm.pc.hardware\n",
       "2  From hilmi Hilmi Eren Subject Re ARMENIA SAYS ...     talk.politics.mideast\n",
       "3  From Guy Dawson Subject Re IDE vs SCSI DMA and...  comp.sys.ibm.pc.hardware\n",
       "4  From Alexander Samuel McDiarmid Subject driver...     comp.sys.mac.hardware"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.loc[:, 'News'] = news_df['News'].apply(data_cleansing)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ sklearn.feature_extraction.text ]**\n",
    "- 데이터 전처리 과정의 tokenization, stopword제거, stemming등을 한번에\n",
    "- Scikit-learn text vector화 모듈\n",
    "- Text관련 처리를 Hyper parameter로 한번에 처리\n",
    "- CountVectorizer, TfidVectorizer 제공\n",
    "- Stemmer 등 NLTK등 외부모듈도 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CountVectorizer**\n",
    "- 문서 집합으로부터 단어의 수를 세어 카운트 행렬을 만듦<br><br>\n",
    "![img](https://ifh.cc/g/5QPJd6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TfidfVectorizer**\n",
    "- 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소하는 방법\n",
    "- TF(Term Frequency) : 문서에서 해당 단어가 얼마나 나왔는지 나타내주는 빈도 수\n",
    "- DF(Document Frequency) : 해당 단어가 있는 문서의 수\n",
    "- IDF(Inverse Document Frequency) 해당 단어가 있는 문서의 수가 높아질 수록 가중치를 축소해주기 위해 역수 취해줌\n",
    "    - log(N/(1 + DF))\n",
    "        - N : 전체 문서의 수\n",
    "- TF-IDF : TF * IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **CustomizedVectorizer** - StemmedCounterVectorizer, StemmedTfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Stemmer 적용하기 ]**\n",
    "- Stemmer 또는 Lemma를 적용하기 위해 nltk사용\n",
    "- 기존 Vectorizer의 상속을 받아 새로운 클래스 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk 설치 안되어있으면 커맨드창에서\n",
    "# !conda install nltk\n",
    "# 혹은\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmer 사용예<br>\n",
    "- look의 어간으로 stem처리하여 저장됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['look', 'look', 'look']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import stem\n",
    "stmmer = stem.SnowballStemmer(\"english\")\n",
    "sentence = 'looking looks looked'\n",
    "[stmmer.stem(word) for word in sentence.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 명사와 동사도 구분해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('imag', 'imag', 'imagin')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stmmer.stem(\"images\"), stmmer.stem(\"imaging\"), stmmer.stem(\"imagination\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StemmedCounterVectorizer**\n",
    "- stemming기능을 추가한 Vectorizer 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "enlish_stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "# CountVectorizer 상속받음\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    # stemming 기능 추가\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        # 각각의 문서(doc)를 읽을 때, 단어를 하나씩 뽑아서 stem처리하고 반환해줘라\n",
    "        return lambda doc: (enlish_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'look': 0}\n",
      "{'looking': 1, 'looks': 2, 'looked': 0}\n"
     ]
    }
   ],
   "source": [
    "# 두 Vectorizer의 결과가 약간씩 다름\n",
    "print(StemmedCountVectorizer(min_df=1, stop_words=\"english\").fit([sentence]).vocabulary_)\n",
    "print(CountVectorizer(min_df=1, stop_words=\"english\").fit([sentence]).vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer 구현\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "enlish_stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (enlish_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Scikit-learn Pipeline 모듈 ]**\n",
    "- Train/ Test데이터에 적용할 과정들을 동일하게\n",
    "- Vectorizer를 적용할 때는 Train/Test에 한번에\n",
    "- 여러번 실험할 때 거쳐야 하는 과정을 정의\n",
    "- Scikit-learn의 pipeline모듈 사용<br><br>\n",
    "![img](https://ifh.cc/g/HNvDjS.png)<br><br>\n",
    "![img](https://ifh.cc/g/GbShvQ.png)\n",
    "- T1,T2라는 과정에 X,X1을 피팅시켜 저장하고 최종적으로 Classifer(Model)을 만들어냄\n",
    "- 그 이후에 test데이터를 만나면 만들었던 Model로 predict를 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gridsearch**\n",
    "- Hyperparameter를 search할 때 사용\n",
    "- Hyperparameter 변수 조합 자동 생성하여 좋을 조합을 찾아내자\n",
    "- Pipeline과 합쳐서 estimator별로 parameter설정\n",
    "- GridsearchCV(CrossValidation)등 샘플링 class제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline + Gridsearch**<br>\n",
    "\n",
    "![img](https://ifh.cc/g/FPyAlP.png)\n",
    "- pipeline은 평소처럼만들고, 그 안의 vect,clf같은 parameter들을 param_grid에 올수 있는 인자들로 넣어서 최적의 조합을 찾아줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "**Modeling Plan**<br><br>\n",
    "![img](https://ifh.cc/g/5N3HBG.png)\n",
    "- 4개의 Vectorizer X 4개의 Algorithm => 16개의 Pipeline 생성\n",
    "- CV는 시간관계상 2번만 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a')\n",
      "('is', 'a', 'foo')\n",
      "('a', 'foo', 'bar')\n",
      "('foo', 'bar', 'sentences')\n",
      "('bar', 'sentences', 'and')\n",
      "('sentences', 'and', 'i')\n",
      "('and', 'i', 'want')\n",
      "('i', 'want', 'to')\n",
      "('want', 'to', 'ngramize')\n",
      "('to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "n = 6\n",
    "sixgrams = ngrams(sentence.split(), 3)\n",
    "for grams in sixgrams:\n",
    "  print (grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- text들을 tokenize하다보면 데이터가 sparse하게 구성되어서 해당 데이터들을 csr이라는 형태로 저장된다.\n",
    "- 그래서 이 유형의 데이터들을 **.toarray()** 등을 통해 numpy형태로 다르게 핸들링해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(CountVectorizer().fit_transform([sentence]).toarray())\n",
    "CountVectorizer().fit_transform([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GausianNB는 csr형태로 데이터를 못받고 dense한 형태의 데이터만 받을수 있어서 해당 내용에만 transform할 수 있도록 처리\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "class DenseTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pipeline(memory=None,\n",
       "      steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(memory=None,\n",
       "      steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "   ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False))]),\n",
       " Pipeline(memory=None,\n",
       "      steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_i...   vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(memory=None,\n",
       "      steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_i...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False))]),\n",
       " Pipeline(memory=None,\n",
       "      steps=[('stemmedcountvectorizer', StemmedCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "             dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "             lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "             ngram_range=(1, 1), preprocessor...e, vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(memory=None,\n",
       "      steps=[('stemmedcountvectorizer', StemmedCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "             dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "             lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "             ngram_range=(1, 1), preprocessor...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False))]),\n",
       " Pipeline(memory=None,\n",
       "      steps=[('stemmedtfidfvectorizer', StemmedTfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "             dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "             lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "             ngram_range=(1, 1), norm='l2', p...e, vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(memory=None,\n",
       "      steps=[('stemmedtfidfvectorizer', StemmedTfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "             dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "             lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "             ngram_range=(1, 1), norm='l2', p...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False))])]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB,GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# vectorizer, algorithms종류들을 일단 리스트로 저장\n",
    "vectorizer = [CountVectorizer(), TfidfVectorizer(), StemmedCountVectorizer(), StemmedTfidfVectorizer()]\n",
    "# algorithms = [BernoulliNB(), MultinomialNB(), GaussianNB(), LogisticRegression()]\n",
    "algorithms = [MultinomialNB(), LogisticRegression()]\n",
    "\n",
    "pipelines  = [] \n",
    "\n",
    "# 경우의 수 만들어주기. 총 16개 생성\n",
    "import itertools\n",
    "for case in list(itertools.product(vectorizer, algorithms)):\n",
    "    # GaussianNB형태만 다르게 학습하려고 if문\n",
    "    if isinstance(case[1], GaussianNB):\n",
    "        case = list(case)\n",
    "        case.insert(1,  DenseTransformer())\n",
    "    pipelines.append(make_pipeline(*case))\n",
    "pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vectorizer Common params 경우의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvectorizer': {'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'countvectorizer__max_df': array([0.6       , 0.66666667, 0.73333333, 0.8       ]),\n",
       "  'countvectorizer__min_df': array([0.]),\n",
       "  'countvectorizer__lowercase': [True, False],\n",
       "  'countvectorizer__stop_words': ['english']},\n",
       " 'tfidfvectorizer': {'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'tfidfvectorizer__max_df': array([0.6       , 0.66666667, 0.73333333, 0.8       ]),\n",
       "  'tfidfvectorizer__min_df': array([0.]),\n",
       "  'tfidfvectorizer__lowercase': [True, False],\n",
       "  'tfidfvectorizer__stop_words': ['english']},\n",
       " 'stemmedcountvectorizer': {'stemmedcountvectorizer__ngram_range': [(1, 1),\n",
       "   (1, 3)],\n",
       "  'stemmedcountvectorizer__max_df': array([0.6       , 0.66666667, 0.73333333, 0.8       ]),\n",
       "  'stemmedcountvectorizer__min_df': array([0.]),\n",
       "  'stemmedcountvectorizer__lowercase': [True, False],\n",
       "  'stemmedcountvectorizer__stop_words': ['english']},\n",
       " 'stemmedtfidfvectorizer': {'stemmedtfidfvectorizer__ngram_range': [(1, 1),\n",
       "   (1, 3)],\n",
       "  'stemmedtfidfvectorizer__max_df': array([0.6       , 0.66666667, 0.73333333, 0.8       ]),\n",
       "  'stemmedtfidfvectorizer__min_df': array([0.]),\n",
       "  'stemmedtfidfvectorizer__lowercase': [True, False],\n",
       "  'stemmedtfidfvectorizer__stop_words': ['english']}}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_params = [(1,1),(1,3)]\n",
    "stopword_params = [\"english\"]\n",
    "lowercase_params = [True, False]\n",
    "# min(max)_df_params : 단어 등장빈도가 최소 몇%에서 최대 몇%까지만 피쳐생성\n",
    "# 아래 경우에는 최소 10%에서 80%까지만 피쳐로 친다.\n",
    "# linspace : 0.6에서 0.8까지를 4개의 구간으로 잘라서 4가지 linspace를 만들고 경우의 수를 만듦\n",
    "\n",
    "max_df_params = np.linspace(0.6, 0.8, num=4) \n",
    "min_df_params = np.linspace(0.0, 0.1, num=1)\n",
    "\n",
    "attributes = {\"ngram_range\":ngrams_params, \"max_df\":max_df_params,\"min_df\":min_df_params,\n",
    "              \"lowercase\":lowercase_params,\"stop_words\":stopword_params}\n",
    "vectorizer_names = [\"countvectorizer\",\"tfidfvectorizer\",\"stemmedcountvectorizer\",\"stemmedtfidfvectorizer\"]\n",
    "vectorizer_params_dict = {}\n",
    "\n",
    "# 위의 attributes, vectorizer_names를 잘 조합해서 여러 종류의 vectorizer만듦\n",
    "for vect_name in vectorizer_names:\n",
    "    vectorizer_params_dict[vect_name] = {}\n",
    "    for key, value in attributes.items():\n",
    "        param_name = vect_name + \"__\" + key\n",
    "        vectorizer_params_dict[vect_name][param_name] =  value\n",
    "\n",
    "vectorizer_params_dict\n",
    "#countervectorizer의 경우만 : ngram경우의수 : 3개\n",
    "#                             max_df_params : 4개\n",
    "#                             min_df_params : 4개\n",
    "#                             lowercase : 2개(True,False)\n",
    "#                             3 x 4 x 4 x 2 : 총 96개의 경우의 수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algorithms parameters 경우의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multinomialnb': {'multinomialnb__alpha': array([1.])},\n",
       " 'logisticregression': [{'logisticregression__multi_class': ['multinomial'],\n",
       "   'logisticregression__solver': ['saga'],\n",
       "   'logisticregression__penalty': ['l1'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},\n",
       "  {'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}]}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm_names = [\"bernoullinb\",\"multinomialnb\",\"gaussiannb\",\"logisticregression\"]\n",
    "algorithm_names = [\"multinomialnb\", \"logisticregression\"]\n",
    "\n",
    "algorithm_params_dict = {}\n",
    "\n",
    "\n",
    "# Algorithms에 따라 조정해줄 parameter들이 조금씩 달라서 따로 지정해줌\n",
    "\n",
    "#'bernoullinb', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))])\n",
    "alpha_params = np.linspace(1.0, 1.0, num=1)\n",
    "for i in range(1):\n",
    "    algorithm_params_dict[algorithm_names[i]] = {\n",
    "    algorithm_names[i]+ \"__alpha\" : alpha_params    \n",
    "    }\n",
    "# algorithm_params_dict[algorithm_names[2]] = {}\n",
    "\n",
    "\n",
    "# LogisticRegression    \n",
    "# multi_class : str, {‘ovr’, ‘multinomial’}, default: ‘ovr’\n",
    "# C : float, default: 1.0\n",
    "# solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’},\n",
    "# n_jobs : int, default: 1\n",
    "# penalty : str, ‘l1’ or ‘l2’, default: ‘l2’\n",
    "\n",
    "# multi_class_params = [\"ovr\", \"multinomial\"]\n",
    "c_params = [0.1,  5.0, 7.0, 10.0, 15.0, 20.0, 100.0]\n",
    "\n",
    "\n",
    "\n",
    "algorithm_params_dict[algorithm_names[1]] = [{\n",
    "    \"logisticregression__multi_class\" : [\"multinomial\"],\n",
    "    \"logisticregression__solver\" : [\"saga\"],\n",
    "    \"logisticregression__penalty\" : [\"l1\"],\n",
    "    \"logisticregression__C\" : c_params\n",
    "    },{\n",
    "    \"logisticregression__multi_class\" : [\"ovr\"],\n",
    "    \"logisticregression__solver\" : ['liblinear'],\n",
    "    \"logisticregression__penalty\" : [\"l2\"],\n",
    "    \"logisticregression__C\" : c_params\n",
    "    }\n",
    "    ]\n",
    "algorithm_params_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 경우의 수 다만들었으니 pipeline에 각각의 알고리즘들을 넣어주자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'countvectorizer__max_df': array([0.6       , 0.66666667, 0.73333333, 0.8       ]),\n",
       "  'countvectorizer__min_df': array([0.]),\n",
       "  'countvectorizer__lowercase': [True, False],\n",
       "  'countvectorizer__stop_words': ['english'],\n",
       "  'multinomialnb__alpha': array([1.])},\n",
       " [{'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'countvectorizer__max_df': array([0.6       , 0.66666667, 0.73333333, 0.8       ]),\n",
       "   'countvectorizer__min_df': array([0.]),\n",
       "   'countvectorizer__lowercase': [True, False],\n",
       "   'countvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},\n",
       "  {'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'countvectorizer__max_df': array([0.6       , 0.66666667, 0.73333333, 0.8       ]),\n",
       "   'countvectorizer__min_df': array([0.]),\n",
       "   'countvectorizer__lowercase': [True, False],\n",
       "   'countvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}],\n",
       " {'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'tfidfvectorizer__max_df': array([0.6       , 0.66666667, 0.73333333, 0.8       ]),\n",
       "  'tfidfvectorizer__min_df': array([0.]),\n",
       "  'tfidfvectorizer__lowercase': [True, False],\n",
       "  'tfidfvectorizer__stop_words': ['english'],\n",
       "  'multinomialnb__alpha': array([1.])},\n",
       " [{'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'tfidfvectorizer__max_df': array([0.6       , 0.66666667, 0.73333333, 0.8       ]),\n",
       "   'tfidfvectorizer__min_df': array([0.]),\n",
       "   'tfidfvectorizer__lowercase': [True, False],\n",
       "   'tfidfvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},\n",
       "  {'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'tfidfvectorizer__max_df': array([0.6       , 0.66666667, 0.73333333, 0.8       ]),\n",
       "   'tfidfvectorizer__min_df': array([0.]),\n",
       "   'tfidfvectorizer__lowercase': [True, False],\n",
       "   'tfidfvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}],\n",
       " {'stemmedcountvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'stemmedcountvectorizer__max_df': array([0.6       , 0.66666667, 0.73333333, 0.8       ]),\n",
       "  'stemmedcountvectorizer__min_df': array([0.]),\n",
       "  'stemmedcountvectorizer__lowercase': [True, False],\n",
       "  'stemmedcountvectorizer__stop_words': ['english'],\n",
       "  'multinomialnb__alpha': array([1.])},\n",
       " [{'stemmedcountvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'stemmedcountvectorizer__max_df': array([0.6       , 0.66666667, 0.73333333, 0.8       ]),\n",
       "   'stemmedcountvectorizer__min_df': array([0.]),\n",
       "   'stemmedcountvectorizer__lowercase': [True, False],\n",
       "   'stemmedcountvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},\n",
       "  {'stemmedcountvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'stemmedcountvectorizer__max_df': array([0.6       , 0.66666667, 0.73333333, 0.8       ]),\n",
       "   'stemmedcountvectorizer__min_df': array([0.]),\n",
       "   'stemmedcountvectorizer__lowercase': [True, False],\n",
       "   'stemmedcountvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}],\n",
       " {'stemmedtfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'stemmedtfidfvectorizer__max_df': array([0.6       , 0.66666667, 0.73333333, 0.8       ]),\n",
       "  'stemmedtfidfvectorizer__min_df': array([0.]),\n",
       "  'stemmedtfidfvectorizer__lowercase': [True, False],\n",
       "  'stemmedtfidfvectorizer__stop_words': ['english'],\n",
       "  'multinomialnb__alpha': array([1.])},\n",
       " [{'stemmedtfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'stemmedtfidfvectorizer__max_df': array([0.6       , 0.66666667, 0.73333333, 0.8       ]),\n",
       "   'stemmedtfidfvectorizer__min_df': array([0.]),\n",
       "   'stemmedtfidfvectorizer__lowercase': [True, False],\n",
       "   'stemmedtfidfvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},\n",
       "  {'stemmedtfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'stemmedtfidfvectorizer__max_df': array([0.6       , 0.66666667, 0.73333333, 0.8       ]),\n",
       "   'stemmedtfidfvectorizer__min_df': array([0.]),\n",
       "   'stemmedtfidfvectorizer__lowercase': [True, False],\n",
       "   'stemmedtfidfvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}]]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_params= []\n",
    "for case in list(itertools.product(vectorizer_names, algorithm_names)):\n",
    "    vect_params = vectorizer_params_dict[case[0]].copy()\n",
    "    algo_params = algorithm_params_dict[case[1]]\n",
    "    \n",
    "    if isinstance(algo_params, dict):\n",
    "        # vect_paramsr에 algo_params를 합쳐(update)주고 append\n",
    "        vect_params.update(algo_params)\n",
    "        pipeline_params.append(vect_params)\n",
    "    else:\n",
    "        temp = []\n",
    "        for param in algo_params:\n",
    "            vect_params.update(param)\n",
    "            temp.append(vect_params)\n",
    "        pipeline_params.append(temp)\n",
    "pipeline_params\n",
    "# 출력값 해석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드 결과 해석<br>\n",
    "![img](https://ifh.cc/g/2msoNo.png)\n",
    "- bernoullinb__alpha의 array안에 5개의 경우\n",
    "- countervectorizer__lowercase : 2경우\n",
    "- countervectorizer__max_df : 4경우\n",
    "- countervectorizer__min_df : 4경우\n",
    "- countervectorizer__ngram_range : 3경우\n",
    "- 따라서 5경우 X 96경우 => 480번의 학습이 발생\n",
    "- 거기다 CV=2로 설정했으니 총 960번의 학습이 발생한다.(pipeline_params의 1번째 경우만)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 시간 관계상 1000개의 데이터만 진행\n",
    "X_data = news_df.loc[:, 'News'].tolist()[:1000]\n",
    "y_data = news_df['Target'].tolist()[:1000]\n",
    "y = LabelEncoder().fit_transform(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=Pipeline(memory=None,\n",
      "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), p..., vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
      "       fit_params=None, iid=True, n_jobs=36,\n",
      "       param_grid={'countvectorizer__ngram_range': [(1, 1), (1, 3)], 'countvectorizer__max_df': array([0.6    , 0.66667, 0.73333, 0.8    ]), 'countvectorizer__min_df': array([0.]), 'countvectorizer__lowercase': [True, False], 'countvectorizer__stop_words': ['english'], 'multinomialnb__alpha': array([1.])},\n",
      "       pre_dispatch='2*n_jobs', refit='accuracy',\n",
      "       return_train_score='warn', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done  80 out of  80 | elapsed:   36.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=Pipeline(memory=None,\n",
      "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))]),\n",
      "       fit_params=None, iid=True, n_jobs=36,\n",
      "       param_grid=[{'countvectorizer__ngram_range': [(1, 1), (1, 3)], 'countvectorizer__max_df': array([0.6    , 0.66667, 0.73333, 0.8    ]), 'countvectorizer__min_df': array([0.]), 'countvectorizer__lowercase': [True, False], 'countvectorizer__stop_words': ['english'], 'logisticregression__multi_class': [...ticregression__penalty': ['l2'], 'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}],\n",
      "       pre_dispatch='2*n_jobs', refit='accuracy',\n",
      "       return_train_score='warn', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 224 candidates, totalling 1120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done 128 tasks      | elapsed:  5.1min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 물론 다른metric으로 scoring해도됨\n",
    "scoring = ['accuracy']\n",
    "estimator_results = []\n",
    "for i, (estimator, params) in enumerate(zip(pipelines,pipeline_params)):\n",
    "    n_jobs = 36\n",
    "#     if i+1 % 3 == 0:\n",
    "#         n_jobs = 2\n",
    "    gs_estimator = GridSearchCV(\n",
    "        # verbose : 중간중간 결과보여줄지여부\n",
    "        # n_jobs : 몇개의 cpu코어를 사용할것인가\n",
    "            refit=\"accuracy\", estimator=estimator,param_grid=params, scoring=scoring, cv=5, verbose=1, n_jobs=n_jobs)\n",
    "    print(gs_estimator)\n",
    "\n",
    "    gs_estimator.fit(X_data, y)\n",
    "    estimator_results.append(gs_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각각의 gridsearch의 결과물들을 넣어놓음\n",
    "print(len(estimator_results))\n",
    "estimator_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과들을 Dataframe으로 보기 위해 틀 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "result_df_dict = {}\n",
    "result_attributes = [\"vectorizer\", \"model\", \"accuracy\", \"recall_macro\",\"precision_macro\" , \"min_df\", \n",
    "                     \"lowercase\", \"max_df\", \"binarize\", \"alpha\", \"ngram_range\"\n",
    "                     \"multi_class\", \"penalty\", \"solver\", \"C\"]\n",
    "\n",
    "pieline_list =  list(itertools.product(vectorizer_names, algorithm_names))\n",
    "\n",
    "for att in result_attributes:\n",
    "    result_df_dict[att] = [None for i in range(16)]\n",
    "\n",
    "result_df = DataFrame(result_df_dict)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataframe안의 값을 estimator_results의 값을 활용하여 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, estiamtor in enumerate(estimator_results):\n",
    "    # estiamtor에서 가장 좋았던 estiamto와 그 index\n",
    "    best_estimator = estiamtor.best_estimator_\n",
    "    best_index = estiamtor.best_index_\n",
    "    result_df_dict[\"vectorizer\"][i] = pieline_list[i][0]\n",
    "    result_df_dict[\"model\"][i] = pieline_list[i][1]\n",
    "    result_df_dict[\"accuracy\"][i] = estiamtor.best_score_\n",
    "#     result_df_dict[\"recall_micro\"][i] = estiamtor.cv_results_[\"mean_test_recall_micro\"][best_index]\n",
    "#     result_df_dict[\"precision_micro\"][i] = estiamtor.cv_results_[\"mean_test_precision_micro\"][best_index]\n",
    "    for key, value in estiamtor.best_params_.items():\n",
    "        if key.split(\"__\")[1] in result_df_dict:\n",
    "            name = key.split(\"__\")[1]\n",
    "            result_df_dict[key.split(\"__\")[1]][i] = value\n",
    "#     print(estiamtor.best_params_)\n",
    "#     print(a.named_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = DataFrame(result_df_dict, columns=result_attributes)\n",
    "result_df.sort_values(\"accuracy\",ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
