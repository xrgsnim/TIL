{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logistic Regreesion extended**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multicalss Classification**\n",
    "- 3개 이상의 class로 분류를 하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**[ 개념정리 ]**<br><br>\n",
    "**Multiclass classification**\n",
    "- 두 개 이상의 클래스를 가진 분류 작업\n",
    "- 오렌지, 사과, 또는 배.. 이런느낌\n",
    "- **중복 선택 불가** -> [1 0 0] 가능, [1 1 0] 불가\n",
    "<br><br>\n",
    "\n",
    "**Multilabel classification**\n",
    "- 상호 배타적이지 않은 속성을 예측\n",
    "- **중복 선택 가능**한 분류 -> [1 1 0] 가능\n",
    "- 신문기사 분류 : 야구선수 - 연예인 결혼 -> 스포츠 / 연예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**[ Approach ]**<br><br>\n",
    "**One-vs-All**\n",
    "- m개의 class가 존재할 때, 클래스마다 classifier 생성\n",
    "- 1개의 class와 그 외'로 classifier생성하며 이것을 m개의 class만큼 진행\n",
    "- 이 부분에 대해서 디테일하게 공부\n",
    "<br><br>\n",
    "\n",
    "**One-vs-One**\n",
    "- 2쌍씩의 Class마다 Clasifier를 생성, 최종 선택 시 Classifier선택의 투표를 통해 결정\n",
    "- 총 m(m-1) / 2 개 만큼의 Classifier 생성, 정확도 Up, Classifier를 많이 만들어서 그만큼 속도는 Down\n",
    "- 앙상블 기법과 거의 흡사\n",
    "- 이부분은 skitlearn으로만 공부"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid function for multiclass**<br><br>\n",
    "**One vs All Approach**\n",
    "- m개의 classifier함수 $h_m(x; \\theta)$ 생성\n",
    "- $h_m(x; \\theta)$의 확률 값 중 가장 높은 값을 가진 m을 선택\n",
    "- **각 $h_m(x; \\theta)$의 확률 합이 1이상이라는 문제점이 생김**\n",
    "- 예시) $h_1 = 0.2$, $h_2 = 0.5$, $h_3 = 0.4$ $\\cdots$ 이렇게되면 $h_m(x; \\theta)$의 확률 합이 1을 넘게됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Softmax function for multiclass**\n",
    "- 모든 class의 확률을 1로 Generalize함\n",
    "> #### $\\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for $j = 1, 2, 3, \\cdots, K$\n",
    "> $\\sum_{j=1}^K \\sigma(z)_j = \\sum_{j=1}^K P_j = 1$<br><br>\n",
    "> $where$ : $\\sigma(z)_j$ = probability of class j<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $K$ = last classification class\n",
    "\n",
    "- 예시) 개or 고양이or 사자가 될 확률 i = [0.3, 0.2, 0.5]\n",
    "- 이렇게 각 사건이 일어날 확률의 합이 1이되도록 Generalize해줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Softmax Function**<br>\n",
    "Class가 두 개일 때\n",
    "> ##### $\\frac{P_j}{1 - P_j} \\Rightarrow logit(P_j) = log_e (\\frac{P_j}{1 - P_j}) = z = \\theta^T x$\n",
    "\n",
    "Class가 K개 일 떄\n",
    "> ##### $\\frac{P_j}{P_K} \\Rightarrow logit(P_j) = log_e (\\frac{P_j}{P_K}) = z_j = x^T \\theta_j$\n",
    "- $P_K$ : 마지막 사건이 일어날 확률\n",
    "- 거기에 logit function 취해서 $z_j$구함.\n",
    "- $z_j$ : 사건 j가 일어날 확률. $x$와 $\\theta_j$의 linear combination으로 구할수있음\n",
    "- $\\theta$가 j개 만큼 있을 때, 각각을 x와 계산하여 가장 확률값이 큰걸 뱉을 수 있도록 식 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 식을 아래와 같이 정리\n",
    ">##### $log_e (\\frac{P_j}{P_K}) = z_j \\Rightarrow \\frac{P_j}{P_K} = e^{z_j}$\n",
    "- 양쪽에 sum\n",
    ">##### $ \\Rightarrow \\sum_{j=1}^K \\frac{P_j}{P_K} = \\sum_{j=1}^K e^{z_j}$\n",
    "- $P_K$는 상수값이기 때문에 sum 밖으로 빼준다.\n",
    ">##### $ \\Rightarrow \\frac{1}{P_K} \\sum_{j=1}^K P_j = \\sum_{j=1}^K e^{z_j}$\n",
    "- $\\sum_{j=1}^K P_j = 1$이기 때문에 아래처럼 표현.\n",
    ">##### $ \\Rightarrow \\frac{1}{P_K} * 1 = \\sum_{j=1}^K e^{z_j}$\n",
    ">##### $ \\Rightarrow P_K = \\frac{1}{\\sum_{j=1}^K e^{z_j}}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $(z_j = x \\theta_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$ \\frac{P_j}{P_K} = e^{z_j} $\n",
    ">##### $ \\Rightarrow \\frac{P_j}{\\frac{1}{\\sum_{j=1}^K e^{z_j}}} = e^{z_j}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ($\\because P_K = \\frac{1}{\\sum_{j=1}^K e^{z_j}}$)<br>\n",
    ">##### $\\therefore P_j = \\frac{e^{z_j}}{\\sum_{j=1}^K e^{z_j}}$\n",
    "- $P_j$ : 어떤 사건이 일어날 확률\n",
    "- 예를들어,$P_j$는 개,고양이,사자(j=1,2,3)일 확률이라 하고 각 확률값은 $e^{z_j}$의 총합분의 $e^{z_j}$가 된다.\n",
    "- j=1 일때라면, j=1부터 K까지 $e^z$들의 합 분의 $e^{z_1}$이 된다.\n",
    "- 그리하여, 최종적으로 각 확률들의 총합이 1이되는 softmax함수를 유도할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $P_j = \\frac{e^{z_j}}{\\sum_{j=1}^K e^{z_j}} = \\frac{e^{z_j}}{\\sum_{j=1}^K e^{x^T \\theta_j}}$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;($\\because z_j = x^T \\theta_j$)\n",
    "\n",
    "\n",
    "$\\theta = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_j \\end{bmatrix} = \\begin{bmatrix} w_{1 0} & w_{1 1} & \\cdots & w_{1 i} \\\\ w_{2 0} & w_{2 1} & \\cdots & w_{2 i} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_{k 0} & w_{k 1} & \\cdots & w_{k i} \\end{bmatrix}$\n",
    "- $\\theta_j$에서 j : 클래스의 갯수\n",
    "- 따라서, $w_{1 0}, w_{1 1}, \\cdots, w_{1 i}$ : 첫번째 class일 때 weight의 값\n",
    "- 예) class가 3개고 weight가 7개 일 때 구해야 하는 weight들은 $w_0$까지 포함하여 총 25개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### **Softmax regression** - Multiclass Classifier\n",
    "- Softmax function 학습 $\\rightarrow$ 결국은 $\\theta$의 학습\n",
    "- 클래스마다 $\\theta$가 존재함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $h_\\theta (x) = \\begin{bmatrix} P(y = 1|x; \\theta) \\\\ P(y = 2|x; \\theta) \\\\ \\vdots \\\\ P(y = K|x; \\theta) \\end{bmatrix} = \\frac{1}{\\sum_{j=1}^K exp(\\theta^{(j)^T x})} \\begin{bmatrix} exp(\\theta^{(1)^T x}) \\\\ exp(\\theta^{(2)^T x}) \\\\ \\vdots \\\\ exp(\\theta^{(K)^T x}) \\end{bmatrix}$\n",
    "- 각 클래스마다의 $\\theta$값을 구해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 따라서!, 이번 챕터의 목적 : 확률의 최대화 !\n",
    "- 확률을 최대화할 $\\theta$를 찾자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Maximum Likelihood Estimation ]**\n",
    "<br><br>\n",
    "> $argmax_\\theta \\prod_{i=1}^m P(y^{(i)}|x^{(i)};\\theta)$\n",
    "- y의 값과 x의 값이 주어졌을 때, 전체 확률 P를 최대화 할 수 있는 $\\theta$를 찾자(데이터의 갯수 : m개)\n",
    "\n",
    "> $p^y(1-p)^{1-y} \\Rightarrow p^{v_1}_1 p^{v_2}_2  \\cdots p^{v_j}_j$\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "> where $ v_j=\n",
    "> \\begin{cases}\n",
    "> 1 \\ \\ \\ \\ \\ if \\ y = v_j\\\\\n",
    "> 0 \\ \\ \\ \\ \\ if \\ y \\ne v_j\n",
    "> \\end{cases}$\n",
    "- class가 binary한 상황에서는 왼쪽처럼 표현했지만 이제 multi class니까 오른쪽처럼 표현을 바꿔줌\n",
    "- 클래스 1($p_1$)이 일어나면 그것의 지수인 $v_1 = 1$ 아니면 $v_1 = 0$\n",
    "\n",
    "> $argmax_\\theta \\prod_{i=1}^n p(x^{(i)}; \\theta^{(i)})^{(i)} (1 - p(x^{i}); > \\theta^{(i)})^{(1 - y^{(i)})}$\n",
    "- class가 binary한 상황에서의 최대화 식. 위의 바뀐 p표기들이 여기에 대입되어 multi class 상황에서의 새로운 식을 만들것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br><br>**[ Negative Log-Likelihood ]**\n",
    "\n",
    "> ##### $L = \\prod_{i=1}^m P(y^{(i)} | x^{(i)}; \\theta_j) = \\prod_{i=1}^m \\prod_{j=1}^K p^{(i)^{v_{ij}}}$\n",
    "> $p_j^{(i)} = \\frac{e^{x^{(i)T} \\theta_j}}{\\sum_{j=1}^K e^{x^{(i)T} \\theta_j}}$ &nbsp;&nbsp;,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "where $v_{ij} =\n",
    "\\begin{cases}\n",
    "1\\ \\ \\  \\ \\  \\ \\;if \\ \\;y^{(i)} is \\ label \\ j\\\\\n",
    "0\\ \\ \\  \\ \\  \\ \\;if \\ \\;y^{(i)} is \\ NOT \\ label \\ j\\\\\n",
    "\\end{cases}$\n",
    "- m개의 데이터와 K개의 클래스에서의 모든 p값들의 곱\n",
    "- i번째 데이터가 j번째 클래스이면 1 아니면 0\n",
    "- 그래서 예를들어 클래스 1,2,3에서 데이터 1이 [1, 0, 0] 이렇게 된다면, $v_{1 1} = 1$, $v_{1 2} = 0$, $v_{1 3} = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확률을 최대화 하고자 했으나 그 반대 개념인 loss는 최소화 해줘야하기 때문에 -(minus) 붙인다.\n",
    "- 확률들의 곱을 구하기는 어려움이 따라서 log를 씌워준다.\n",
    "- Negative Log-Likelihood\n",
    "> $-log L = -log \\prod_{i=1}^m \\prod_{j=1}^K p^{(i)^{v_{ij}}} = -\\sum_{i=1}^m \\sum_{j=1}^K v_{ij} log p^{(i)}_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 따라서, 아래식을 최소화한다.\n",
    "> $l = -\\sum_{i=1}^m \\sum_{j=1}^K v_ij log p_j^{i}$\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "> where $p_j^{(i)} = \\frac{e^{z^{(i)}_j}}{\\sum_{j=1}^K e^{z_j^{(i)}}}$\n",
    "\n",
    "- 식 $l$을 최소화하는 weight값을 찾자\n",
    "- $\\frac{\\partial l}{\\partial p_j} \\frac{\\partial p_j}{\\partial z_j} \\frac{\\partial z_j}{\\partial \\theta_j}$\n",
    "- 사실 식 $l$을 p에 관하여 미분해야하는데 p는 z로 이루어져있고 z는 또 theta로 이루어져 있기 때문에 위와같이 체인룰에 의하여 결국 식 $l$을 theta로 미분해주는 꼴이된다.\n",
    "\n",
    "- 우선 식 $l$을 z로 미분해보자<br>\n",
    "$\\frac{\\partial l}{\\partial z}$\n",
    "-> 1) derivate of Negative-Log Likelihood <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-> 2) derivate of Softmax Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>**[ Minimize Cost Funtion ]**\n",
    "\n",
    "> ##### $\\frac{\\partial l}{\\partial z_i} = - \\sum_{j=1}^K v_{ij} \\frac{\\partial log p_j}{\\partial z_i} = -\\sum_{j=1}^K v_{ij} \\frac{1}{p_j} \\frac{\\partial p_j}{\\partial z_i}$\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "> ($\\because \\frac{d log_e f(x)}{dx} = \\frac{1}{f(x)} \\frac{df(x)}{dx}$)\n",
    "- 여기서 $z_i$의 $i$는 몇번째 데이터할 때 $i$가 아니고 임의의 i, 그냥 미지수\n",
    ">### $\\frac{\\partial z_c}{\\partial p_j} = \\frac{\\partial \\frac{e^{z_j}}{\\sum_{k-1}^K e^{z_k}}}{\\partial z_c} \\Rightarrow f^{'}_j = \\frac{g^{'}_j h_j - h^{'}_j g_j}{[h]^2}$\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "> (where $g_j = e^{z_j}$, $h_j = \\sum_{k=1}^K e^{z_k}$)\n",
    "- 이때 c는 class, c와 j도 그냥 미지수 둘이 같을수도 다를수도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> if $c = j$ <br>\n",
    "> #### $\\frac{\\partial p_j}{\\partial z_c} = \\frac{\\partial \\frac{e^{z_j}}{\\sum_{k-1}^K e^{z_k}}}{\\partial z_c} = \\frac{e^{z_j}h_j - e^{z_c}e^{z_j}}{[h_j]^2} = \\frac{e^{z_j}}{h_j} \\frac{h_j - e^{z_c}}{h_j} = p_j(1 - p_j)$<br>\n",
    "> $\\because p_j = \\frac{e^{z_j}}{\\sum_{j=1}^K e^{z_j}}$\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "> $\\frac{dy}{dx} = \\frac{d(e^{x^2})}{x} = e^{x^2}(\\frac{dx^2}{dx}) = 2x(e^{x^2})$\n",
    "\n",
    "- 여기서 $e^{z_j} = g_j$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   $\\sum_{k-1}^K e^{z_k} = h_j$로 치환해서 정리했음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> if $c = j$ <br>\n",
    "> #### $\\frac{\\partial p_j}{\\partial z_c} = \\frac{\\partial \\frac{e^{z_j}}{\\sum_{k-1}^K e^{z_k}}}{\\partial z_c} = \\frac{0 - e^{z_c}e^{z_j}}{[h_j]^2} = - \\frac{e^{z_j}}{h_j} \\frac{e^{z_c}}{h_j} = -p_j p_c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 두 경우를 하나의 식으로 정리해보자\n",
    "> ##### $\\frac{\\partial l}{\\partial z_i} = - \\sum_{j=1}^K v_{ij} \\frac{1}{p_j} \\frac{\\partial p_j}{\\partial z_i} = -\\frac{v_i}{p_i} \\frac{\\partial p_i}{\\partial z_i} - \\sum_{j \\ne i}^K \\frac{v_j}{p_j} \\frac{\\partial p_j}{\\partial z_i}$\n",
    "- $-\\frac{v_i}{p_i} \\frac{\\partial p_i}{\\partial z_i}$ 부분만 i=j일 때고 그 뒤의 sum식은 i $\\ne$ j 일때 \n",
    "- i는 c라고 생각해도 무방함, 그렇게하면 더 이해가 쉬움\n",
    "> ##### $= -\\frac{v_i}{p_i}p_i(1-p_i) - \\sum_{j \\ne i}^K \\frac{v_j}{p_j} (-p_j p_i)$\n",
    "- 같을 때와 같지 않을 때 각각의 미분식은 위에서의 결과 활용\n",
    "> ##### $= -v_i + v_i p_i + \\sum_{j \\ne i}^K v_j p_i = -v_i + \\sum_{j=1}^K v_j p_i$\n",
    "- 식 정리하면 이렇게 정리되고 sum식의 j $\\ne$ i에 j = i 부분만 넣어주면 되니까 앞부분의 $v_i p_i$에서 $v_i$만 때서 sum식안에 넣어준다.\n",
    "- 그렇게 해서 뒤의 식으로 정리\n",
    "> $p_i - v_i$\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "> ($\\because \\sum_{j=1}^K v_j = 1$)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('hyeooi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c36318f1f175f6468c37ab31a9400557c0f81fa065037d590afb13b0d6bda87f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
