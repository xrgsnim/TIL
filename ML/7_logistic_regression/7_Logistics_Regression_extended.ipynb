{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logistic Regreesion extended**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multicalss Classification**\n",
    "- 3개 이상의 class로 분류를 하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**[ 개념정리 ]**<br><br>\n",
    "**Multiclass classification**\n",
    "- 두 개 이상의 클래스를 가진 분류 작업\n",
    "- 오렌지, 사과, 또는 배.. 이런느낌\n",
    "- **중복 선택 불가** -> [1 0 0] 가능, [1 1 0] 불가\n",
    "<br><br>\n",
    "\n",
    "**Multilabel classification**\n",
    "- 상호 배타적이지 않은 속성을 예측\n",
    "- **중복 선택 가능**한 분류 -> [1 1 0] 가능\n",
    "- 신문기사 분류 : 야구선수 - 연예인 결혼 -> 스포츠 / 연예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**[ Approach ]**<br><br>\n",
    "**One-vs-All**\n",
    "- m개의 class가 존재할 때, 클래스마다 classifier 생성\n",
    "- 1개의 class와 그 외'로 classifier생성하며 이것을 m개의 class만큼 진행\n",
    "- 이 부분에 대해서 디테일하게 공부\n",
    "<br><br>\n",
    "\n",
    "**One-vs-One**\n",
    "- 2쌍씩의 Class마다 Clasifier를 생성, 최종 선택 시 Classifier선택의 투표를 통해 결정\n",
    "- 총 m(m-1) / 2 개 만큼의 Classifier 생성, 정확도 Up, Classifier를 많이 만들어서 그만큼 속도는 Down\n",
    "- 앙상블 기법과 거의 흡사\n",
    "- 이부분은 skitlearn으로만 공부"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid function for multiclass**<br><br>\n",
    "**One vs All Approach**\n",
    "- m개의 classifier함수 $h_m(x; \\theta)$ 생성\n",
    "- $h_m(x; \\theta)$의 확률 값 중 가장 높은 값을 가진 m을 선택\n",
    "- **각 $h_m(x; \\theta)$의 확률 합이 1이상이라는 문제점이 생김**\n",
    "- 예시) $h_1 = 0.2$, $h_2 = 0.5$, $h_3 = 0.4$ $\\cdots$ 이렇게되면 $h_m(x; \\theta)$의 확률 합이 1을 넘게됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Softmax function for multiclass**\n",
    "- 모든 class의 확률을 1로 Generalize함\n",
    "> #### $\\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for $j = 1, 2, 3, \\cdots, K$\n",
    "> $\\sum_{j=1}^K \\sigma(z)_j = \\sum_{j=1}^K P_j = 1$<br><br>\n",
    "> $where$ : $\\sigma(z)_j$ = probability of class j<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $K$ = last classification class\n",
    "\n",
    "- 예시) 개or 고양이or 사자가 될 확률 i = [0.3, 0.2, 0.5]\n",
    "- 이렇게 각 사건이 일어날 확률의 합이 1이되도록 Generalize해줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Softmax Function**<br>\n",
    "Class가 두 개일 때\n",
    "> ##### $\\frac{P_j}{1 - P_j} \\Rightarrow logit(P_j) = log_e (\\frac{P_j}{1 - P_j}) = z = \\theta^T x$\n",
    "\n",
    "Class가 K개 일 떄\n",
    "> ##### $\\frac{P_j}{P_K} \\Rightarrow logit(P_j) = log_e (\\frac{P_j}{P_K}) = z_j = x^T \\theta_j$\n",
    "- $P_K$ : 마지막 사건이 일어날 확률\n",
    "- 거기에 logit function 취해서 $z_j$구함.\n",
    "- $z_j$ : 사건 j가 일어날 확률. $x$와 $\\theta_j$의 linear combination으로 구할수있음\n",
    "- $\\theta$가 j개 만큼 있을 때, 각각을 x와 계산하여 가장 확률값이 큰걸 뱉을 수 있도록 식 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 식을 아래와 같이 정리\n",
    ">##### $log_e (\\frac{P_j}{P_K}) = z_j \\Rightarrow \\frac{P_j}{P_K} = e^{z_j}$\n",
    "- 양쪽에 sum\n",
    ">##### $ \\Rightarrow \\sum_{j=1}^K \\frac{P_j}{P_K} = \\sum_{j=1}^K e^{z_j}$\n",
    "- $P_K$는 상수값이기 때문에 sum 밖으로 빼준다.\n",
    ">##### $ \\Rightarrow \\frac{1}{P_K} \\sum_{j=1}^K P_j = \\sum_{j=1}^K e^{z_j}$\n",
    "- $\\sum_{j=1}^K P_j = 1$이기 때문에 아래처럼 표현.\n",
    ">##### $ \\Rightarrow \\frac{1}{P_K} * 1 = \\sum_{j=1}^K e^{z_j}$\n",
    ">##### $ \\Rightarrow P_K = \\frac{1}{\\sum_{j=1}^K e^{z_j}}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $(z_j = x \\theta_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$ \\frac{P_j}{P_K} = e^{z_j} $\n",
    ">##### $ \\Rightarrow \\frac{P_j}{\\frac{1}{\\sum_{j=1}^K e^{z_j}}} = e^{z_j}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ($\\because P_K = \\frac{1}{\\sum_{j=1}^K e^{z_j}}$)<br>\n",
    ">##### $\\therefore P_j = \\frac{e^{z_j}}{\\sum_{j=1}^K e^{z_j}}$\n",
    "- $P_j$ : 어떤 사건이 일어날 확률\n",
    "- 예를들어,$P_j$는 개,고양이,사자(j=1,2,3)일 확률이라 하고 각 확률값은 $e^{z_j}$의 총합분의 $e^{z_j}$가 된다.\n",
    "- j=1 일때라면, j=1부터 K까지 $e^z$들의 합 분의 $e^{z_1}$이 된다.\n",
    "- 그리하여, 최종적으로 각 확률들의 총합이 1이되는 softmax함수를 유도할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $P_j = \\frac{e^{z_j}}{\\sum_{j=1}^K e^{z_j}} = \\frac{e^{z_j}}{\\sum_{j=1}^K e^{x^T \\theta_j}}$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;($\\because z_j = x^T \\theta_j$)\n",
    "\n",
    "\n",
    "$\\theta = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_j \\end{bmatrix} = \\begin{bmatrix} w_{1 0} & w_{1 1} & \\cdots & w_{1 i} \\\\ w_{2 0} & w_{2 1} & \\cdots & w_{2 i} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_{k 0} & w_{k 1} & \\cdots & w_{k i} \\end{bmatrix}$\n",
    "- $\\theta_j$에서 j : 클래스의 갯수\n",
    "- 따라서, $w_{1 0}, w_{1 1}, \\cdots, w_{1 i}$ : 첫번째 class일 때 weight의 값\n",
    "- 예) class가 3개고 weight가 7개 일 때 구해야 하는 weight들은 $w_0$까지 포함하여 총 25개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### **Softmax regression** - Multiclass Classifier\n",
    "- Softmax function 학습 $\\rightarrow$ 결국은 $\\theta$의 학습\n",
    "- 클래스마다 $\\theta$가 존재함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $h_\\theta (x) = \\begin{bmatrix} P(y = 1|x; \\theta) \\\\ P(y = 2|x; \\theta) \\\\ \\vdots \\\\ P(y = K|x; \\theta) \\end{bmatrix} = \\frac{1}{\\sum_{j=1}^K exp(\\theta^{(j)^T x})} \\begin{bmatrix} exp(\\theta^{(1)^T x}) \\\\ exp(\\theta^{(2)^T x}) \\\\ \\vdots \\\\ exp(\\theta^{(K)^T x}) \\end{bmatrix}$\n",
    "- 각 클래스마다의 $\\theta$값을 구해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 따라서!, 이번 챕터의 목적 : 확률의 최대화 !\n",
    "- 확률을 최대화할 $\\theta$를 찾자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Maximum Likelihood Estimation ]**\n",
    "<br><br>\n",
    "> $argmax_\\theta \\prod_{i=1}^m P(y^{(i)}|x^{(i)};\\theta)$\n",
    "- y의 값과 x의 값이 주어졌을 때, 전체 확률 P를 최대화 할 수 있는 $\\theta$를 찾자(데이터의 갯수 : m개)\n",
    "\n",
    "> $p^y(1-p)^{1-y} \\Rightarrow p^{v_1}_1 p^{v_2}_2  \\cdots p^{v_j}_j$\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "> where $ v_j=\n",
    "> \\begin{cases}\n",
    "> 1 \\ \\ \\ \\ \\ if \\ y = v_j\\\\\n",
    "> 0 \\ \\ \\ \\ \\ if \\ y \\ne v_j\n",
    "> \\end{cases}$\n",
    "- class가 binary한 상황에서는 왼쪽처럼 표현했지만 이제 multi class니까 오른쪽처럼 표현을 바꿔줌\n",
    "- 클래스 1($p_1$)이 일어나면 그것의 지수인 $v_1 = 1$ 아니면 $v_1 = 0$\n",
    "\n",
    "> $argmax_\\theta \\prod_{i=1}^n p(x^{(i)}; \\theta^{(i)})^{(i)} (1 - p(x^{i}); > \\theta^{(i)})^{(1 - y^{(i)})}$\n",
    "- class가 binary한 상황에서의 최대화 식. 위의 바뀐 p표기들이 여기에 대입되어 multi class 상황에서의 새로운 식을 만들것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Negative Log-Likelihood ]**\n",
    "\n",
    "> ##### $L = \\prod_{i=1}^m P(y^{(i)} | x^{(i)}; \\theta_j) = \\prod_{i=1}^m \\prod_{j=1}^K p^{(i)^{v_{ij}}}$\n",
    "> $p_j^{(i)} = \\frac{e^{x^{(i)T} \\theta_j}}{\\sum_{j=1}^K e^{x^{(i)T} \\theta_j}}$ &nbsp;&nbsp;,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "where $v_{ij} =\n",
    "\\begin{cases}\n",
    "1\\ \\ \\  \\ \\  \\ \\;if \\ \\;y^{(i)} is \\ label \\ j\\\\\n",
    "0\\ \\ \\  \\ \\  \\ \\;if \\ \\;y^{(i)} is \\ NOT \\ label \\ j\\\\\n",
    "\\end{cases}$\n",
    "- m개의 데이터와 K개의 클래스에서의 모든 p값들의 곱\n",
    "- i번째 데이터가 j번째 클래스이면 1 아니면 0\n",
    "- 그래서 예를들어 클래스 1,2,3에서 데이터 1이 [1, 0, 0] 이렇게 된다면, $v_{1 1} = 1$, $v_{1 2} = 0$, $v_{1 3} = 0$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('hyeooi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c36318f1f175f6468c37ab31a9400557c0f81fa065037d590afb13b0d6bda87f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
