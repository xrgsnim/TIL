{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logistic Regreesion extended**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multicalss Classification**\n",
    "- 3개 이상의 class로 분류를 하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**[ 개념정리 ]**<br><br>\n",
    "**Multiclass classification**\n",
    "- 두 개 이상의 클래스를 가진 분류 작업\n",
    "- 오렌지, 사과, 또는 배.. 이런느낌\n",
    "- **중복 선택 불가** -> [1 0 0] 가능, [1 1 0] 불가\n",
    "<br><br>\n",
    "\n",
    "**Multilabel classification**\n",
    "- 상호 배타적이지 않은 속성을 예측\n",
    "- **중복 선택 가능**한 분류 -> [1 1 0] 가능\n",
    "- 신문기사 분류 : 야구선수 - 연예인 결혼 -> 스포츠 / 연예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**[ Approach ]**<br><br>\n",
    "**One-vs-All**\n",
    "- m개의 class가 존재할 때, 클래스마다 classifier 생성\n",
    "- 1개의 class와 그 외'로 classifier생성하며 이것을 m개의 class만큼 진행\n",
    "- 이 부분에 대해서 디테일하게 공부\n",
    "<br><br>\n",
    "\n",
    "**One-vs-One**\n",
    "- 2쌍씩의 Class마다 Clasifier를 생성, 최종 선택 시 Classifier선택의 투표를 통해 결정\n",
    "- 총 m(m-1) / 2 개 만큼의 Classifier 생성, 정확도 Up, Classifier를 많이 만들어서 그만큼 속도는 Down\n",
    "- 앙상블 기법과 거의 흡사\n",
    "- 이부분은 skitlearn으로만 공부"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid function for multiclass**<br><br>\n",
    "**One vs All Approach**\n",
    "- m개의 classifier함수 $h_m(x; \\theta)$ 생성\n",
    "- $h_m(x; \\theta)$의 확률 값 중 가장 높은 값을 가진 m을 선택\n",
    "- **각 $h_m(x; \\theta)$의 확률 합이 1이상이라는 문제점이 생김**\n",
    "- 예시) $h_1 = 0.2$, $h_2 = 0.5$, $h_3 = 0.4$ $\\cdots$ 이렇게되면 $h_m(x; \\theta)$의 확률 합이 1을 넘게됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Softmax function for multiclass**\n",
    "- 모든 class의 확률을 1로 Generalize함\n",
    "> #### $\\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for $j = 1, 2, 3, \\cdots, K$\n",
    "> $\\sum_{j=1}^K \\sigma(z)_j = \\sum_{j=1}^K P_j = 1$<br><br>\n",
    "> $where$ : $\\sigma(z)_j$ = probability of class j<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $K$ = last classification class\n",
    "\n",
    "- 예시) 개or 고양이or 사자가 될 확률 i = [0.3, 0.2, 0.5]\n",
    "- 이렇게 각 사건이 일어날 확률의 합이 1이되도록 Generalize해줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Softmax Function**<br>\n",
    "Class가 두 개일 때\n",
    "> ##### $\\frac{P_j}{1 - P_j} \\Rightarrow logit(P_j) = log_e (\\frac{P_j}{1 - P_j}) = z = \\theta^T x$\n",
    "\n",
    "Class가 K개 일 떄\n",
    "> ##### $\\frac{P_j}{P_K} \\Rightarrow logit(P_j) = log_e (\\frac{P_j}{P_K}) = z_j = x^T \\theta_j$\n",
    "- $P_K$ : 마지막 사건이 일어날 확률\n",
    "- 거기에 logit function 취해서 $z_j$구함.\n",
    "- $z_j$ : 사건 j가 일어날 확률. $x$와 $\\theta_j$의 linear combination으로 구할수있음\n",
    "- $\\theta$가 j개 만큼 있을 때, 각각을 x와 계산하여 가장 확률값이 큰걸 뱉을 수 있도록 식 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 식을 아래와 같이 정리\n",
    ">##### $log_e (\\frac{P_j}{P_K}) = z_j \\Rightarrow \\frac{P_j}{P_K} = e^{z_j}$\n",
    "- 양쪽에 sum\n",
    ">##### $ \\Rightarrow \\sum_{j=1}^K \\frac{P_j}{P_K} = \\sum_{j=1}^K e^{z_j}$\n",
    "- $P_K$는 상수값이기 때문에 sum 밖으로 빼준다.\n",
    ">##### $ \\Rightarrow \\frac{1}{P_K} \\sum_{j=1}^K P_j = \\sum_{j=1}^K e^{z_j}$\n",
    "- $\\sum_{j=1}^K P_j = 1$이기 때문에 아래처럼 표현.\n",
    ">##### $ \\Rightarrow \\frac{1}{P_K} * 1 = \\sum_{j=1}^K e^{z_j}$\n",
    ">##### $ \\Rightarrow P_K = \\frac{1}{\\sum_{j=1}^K e^{z_j}}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $(z_j = x \\theta_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$ \\frac{P_j}{P_K} = e^{z_j} $\n",
    ">##### $ \\Rightarrow \\frac{P_j}{\\frac{1}{\\sum_{j=1}^K e^{z_j}}} = e^{z_j}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ($\\because P_K = \\frac{1}{\\sum_{j=1}^K e^{z_j}}$)<br>\n",
    ">##### $\\therefore P_j = \\frac{e^{z_j}}{\\sum_{j=1}^K e^{z_j}}$\n",
    "- $P_j$ : 어떤 사건이 일어날 확률\n",
    "- 예를들어,$P_j$는 개,고양이,사자(j=1,2,3)일 확률이라 하고 각 확률값은 $e^{z_j}$의 총합분의 $e^{z_j}$가 된다.\n",
    "- j=1 일때라면, j=1부터 K까지 $e^z$들의 합 분의 $e^{z_1}$이 된다.\n",
    "- 그리하여, 최종적으로 각 확률들의 총합이 1이되는 softmax함수를 유도할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $P_j = \\frac{e^{z_j}}{\\sum_{j=1}^K e^{z_j}} = \\frac{e^{z_j}}{\\sum_{j=1}^K e^{x^T \\theta_j}}$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;($\\because z_j = x^T \\theta_j$)\n",
    "\n",
    "\n",
    "$\\theta = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_j \\end{bmatrix} = \\begin{bmatrix} w_{1 0} & w_{1 1} & \\cdots & w_{1 i} \\\\ w_{2 0} & w_{2 1} & \\cdots & w_{2 i} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_{k 0} & w_{k 1} & \\cdots & w_{k i} \\end{bmatrix}$\n",
    "- $\\theta_j$에서 j : 클래스의 갯수\n",
    "- 따라서, $w_{1 0}, w_{1 1}, \\cdots, w_{1 i}$ : 첫번째 class일 때 weight의 값\n",
    "- 예) class가 3개고 weight가 7개 일 때 구해야 하는 weight들은 $w_0$까지 포함하여 총 25개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### **Softmax regression** - Multiclass Classifier\n",
    "- Softmax function 학습 $\\rightarrow$ 결국은 $\\theta$의 학습\n",
    "- 클래스마다 $\\theta$가 존재함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $h_\\theta (x) = \\begin{bmatrix} P(y = 1|x; \\theta) \\\\ P(y = 2|x; \\theta) \\\\ \\vdots \\\\ P(y = K|x; \\theta) \\end{bmatrix} = \\frac{1}{\\sum_{j=1}^K exp(\\theta^{(j)^T x})} \\begin{bmatrix} exp(\\theta^{(1)^T x}) \\\\ exp(\\theta^{(2)^T x}) \\\\ \\vdots \\\\ exp(\\theta^{(K)^T x}) \\end{bmatrix}$\n",
    "- 각 클래스마다의 $\\theta$값을 구해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 따라서!, 이번 챕터의 목적 : 확률의 최대화 !\n",
    "- 확률을 최대화할 $\\theta$를 찾자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Maximum Likelihood Estimation ]**\n",
    "<br><br>\n",
    "> $argmax_\\theta \\prod_{i=1}^m P(y^{(i)}|x^{(i)};\\theta)$\n",
    "- y의 값과 x의 값이 주어졌을 때, 전체 확률 P를 최대화 할 수 있는 $\\theta$를 찾자(데이터의 갯수 : m개)\n",
    "\n",
    "> $p^y(1-p)^{1-y} \\Rightarrow p^{v_1}_1 p^{v_2}_2  \\cdots p^{v_j}_j$\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "> where $ v_j=\n",
    "> \\begin{cases}\n",
    "> 1 \\ \\ \\ \\ \\ if \\ y = v_j\\\\\n",
    "> 0 \\ \\ \\ \\ \\ if \\ y \\ne v_j\n",
    "> \\end{cases}$\n",
    "- class가 binary한 상황에서는 왼쪽처럼 표현했지만 이제 multi class니까 오른쪽처럼 표현을 바꿔줌\n",
    "- 클래스 1($p_1$)이 일어나면 그것의 지수인 $v_1 = 1$ 아니면 $v_1 = 0$\n",
    "\n",
    "> $argmax_\\theta \\prod_{i=1}^n p(x^{(i)}; \\theta^{(i)})^{(i)} (1 - p(x^{i}); > \\theta^{(i)})^{(1 - y^{(i)})}$\n",
    "- class가 binary한 상황에서의 최대화 식. 위의 바뀐 p표기들이 여기에 대입되어 multi class 상황에서의 새로운 식을 만들것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br><br>**[ Negative Log-Likelihood ]**\n",
    "\n",
    "> ##### $L = \\prod_{i=1}^m P(y^{(i)} | x^{(i)}; \\theta_j) = \\prod_{i=1}^m \\prod_{j=1}^K p^{(i)^{v_{ij}}}$\n",
    "> $p_j^{(i)} = \\frac{e^{x^{(i)T} \\theta_j}}{\\sum_{j=1}^K e^{x^{(i)T} \\theta_j}}$ &nbsp;&nbsp;,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "where $v_{ij} =\n",
    "\\begin{cases}\n",
    "1\\ \\ \\  \\ \\  \\ \\;if \\ \\;y^{(i)} is \\ label \\ j\\\\\n",
    "0\\ \\ \\  \\ \\  \\ \\;if \\ \\;y^{(i)} is \\ NOT \\ label \\ j\\\\\n",
    "\\end{cases}$\n",
    "- m개의 데이터와 K개의 클래스에서의 모든 p값들의 곱\n",
    "- i번째 데이터가 j번째 클래스이면 1 아니면 0\n",
    "- 그래서 예를들어 클래스 1,2,3에서 데이터 1이 [1, 0, 0] 이렇게 된다면, $v_{1 1} = 1$, $v_{1 2} = 0$, $v_{1 3} = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확률을 최대화 하고자 했으나 그 반대 개념인 loss는 최소화 해줘야하기 때문에 -(minus) 붙인다.\n",
    "- 확률들의 곱을 구하기는 어려움이 따라서 log를 씌워준다.\n",
    "- Negative Log-Likelihood\n",
    "> $-log L = -log \\prod_{i=1}^m \\prod_{j=1}^K p^{(i)^{v_{ij}}} = -\\sum_{i=1}^m \\sum_{j=1}^K v_{ij} log p^{(i)}_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 따라서, 아래식을 최소화한다.\n",
    "> $l = -\\sum_{i=1}^m \\sum_{j=1}^K v_ij log p_j^{i}$\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "> where $p_j^{(i)} = \\frac{e^{z^{(i)}_j}}{\\sum_{j=1}^K e^{z_j^{(i)}}}$\n",
    "\n",
    "- 식 $l$을 최소화하는 weight값을 찾자\n",
    "- $\\frac{\\partial l}{\\partial p_j} \\frac{\\partial p_j}{\\partial z_j} \\frac{\\partial z_j}{\\partial \\theta_j}$\n",
    "- 사실 식 $l$을 p에 관하여 미분해야하는데 p는 z로 이루어져있고 z는 또 theta로 이루어져 있기 때문에 위와같이 체인룰에 의하여 결국 식 $l$을 theta로 미분해주는 꼴이된다.\n",
    "\n",
    "- 우선 식 $l$을 z로 미분해보자<br>\n",
    "$\\frac{\\partial l}{\\partial z}$\n",
    "-> 1) derivate of Negative-Log Likelihood <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-> 2) derivate of Softmax Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>**[ Minimize Cost Funtion ]**\n",
    "\n",
    "> ##### $\\frac{\\partial l}{\\partial z_i} = - \\sum_{j=1}^K v_{ij} \\frac{\\partial log p_j}{\\partial z_i} = -\\sum_{j=1}^K v_{ij} \\frac{1}{p_j} \\frac{\\partial p_j}{\\partial z_i}$\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "> ($\\because \\frac{d log_e f(x)}{dx} = \\frac{1}{f(x)} \\frac{df(x)}{dx}$)\n",
    "- 여기서 $z_i$의 $i$는 몇번째 데이터할 때 $i$가 아니고 임의의 i, 그냥 미지수\n",
    ">### $\\frac{\\partial z_c}{\\partial p_j} = \\frac{\\partial \\frac{e^{z_j}}{\\sum_{k-1}^K e^{z_k}}}{\\partial z_c} \\Rightarrow f^{'}_j = \\frac{g^{'}_j h_j - h^{'}_j g_j}{[h]^2}$\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "> (where $g_j = e^{z_j}$, $h_j = \\sum_{k=1}^K e^{z_k}$)\n",
    "- 이때 c는 class, c와 j도 그냥 미지수 둘이 같을수도 다를수도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> if $c = j$ <br>\n",
    "> #### $\\frac{\\partial p_j}{\\partial z_c} = \\frac{\\partial \\frac{e^{z_j}}{\\sum_{k-1}^K e^{z_k}}}{\\partial z_c} = \\frac{e^{z_j}h_j - e^{z_c}e^{z_j}}{[h_j]^2} = \\frac{e^{z_j}}{h_j} \\frac{h_j - e^{z_c}}{h_j} = p_j(1 - p_j)$<br>\n",
    "> $\\because p_j = \\frac{e^{z_j}}{\\sum_{j=1}^K e^{z_j}}$\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "> $\\frac{dy}{dx} = \\frac{d(e^{x^2})}{x} = e^{x^2}(\\frac{dx^2}{dx}) = 2x(e^{x^2})$\n",
    "\n",
    "- 여기서 $e^{z_j} = g_j$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   $\\sum_{k-1}^K e^{z_k} = h_j$로 치환해서 정리했음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> if $c = j$ <br>\n",
    "> #### $\\frac{\\partial p_j}{\\partial z_c} = \\frac{\\partial \\frac{e^{z_j}}{\\sum_{k-1}^K e^{z_k}}}{\\partial z_c} = \\frac{0 - e^{z_c}e^{z_j}}{[h_j]^2} = - \\frac{e^{z_j}}{h_j} \\frac{e^{z_c}}{h_j} = -p_j p_c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 두 경우를 하나의 식으로 정리해보자\n",
    "> ##### $\\frac{\\partial l}{\\partial z_i} = - \\sum_{j=1}^K v_{ij} \\frac{1}{p_j} \\frac{\\partial p_j}{\\partial z_i} = -\\frac{v_i}{p_i} \\frac{\\partial p_i}{\\partial z_i} - \\sum_{j \\ne i}^K \\frac{v_j}{p_j} \\frac{\\partial p_j}{\\partial z_i}$\n",
    "- $-\\frac{v_i}{p_i} \\frac{\\partial p_i}{\\partial z_i}$ 부분만 i=j일 때고 그 뒤의 sum식은 i $\\ne$ j 일때 \n",
    "- i는 c라고 생각해도 무방함, 그렇게하면 더 이해가 쉬움\n",
    "> ##### $= -\\frac{v_i}{p_i}p_i(1-p_i) - \\sum_{j \\ne i}^K \\frac{v_j}{p_j} (-p_j p_i)$\n",
    "- 같을 때와 같지 않을 때 각각의 미분식은 위에서의 결과 활용\n",
    "> ##### $= -v_i + v_i p_i + \\sum_{j \\ne i}^K v_j p_i = -v_i + \\sum_{j=1}^K v_j p_i$\n",
    "- 식 정리하면 이렇게 정리되고 sum식의 j $\\ne$ i에 j = i 부분만 넣어주면 되니까 앞부분의 $v_i p_i$에서 $v_i$만 때서 sum식안에 넣어준다.\n",
    "- 그렇게 해서 뒤의 식으로 정리\n",
    "> $p_i - v_i$\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "> ($\\because \\sum_{j=1}^K v_j = 1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미분값 배웠으니 이제 이값을 업데이트 해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Update weights ]**\n",
    "\n",
    "> $w_{kj} = w_{kj} - \\alpha \\frac{\\partial l}{\\partial w_{kj}}$<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$= w_{kj} - \\alpha \\frac{\\partial l}{\\partial z_k} \\frac{\\partial z_k}{\\partial w_{kj}}$<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$= w_{kj} - \\alpha(p_k - v_k)x_j$<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$= w_{kj} + \\alpha(v_k - p_k)x_j$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ \\Rightarrow w_{kj} = w_{kj} + \\alpha \\sum_{i=1}^n (v^{(i)}_k - p^{(i)}_k)x^{(i)}_j$\n",
    "\n",
    "- w : weight\n",
    "- k : 클래스, j : 변수 (예 : $w_{kj} = w_{13}$ : 번째 클래스에서 3번째 변수)\n",
    "- 각각의 weight값들에 대해서 simultaneous하게 업데이트\n",
    "- $(v^{(i)}_k - p^{(i)}_k)x^{(i)}_j$부분 예시 : 사자 고양이 강아지 = [1, 0, 0]로 되어있으면 $v^{(i)}_0 = 1$이고 $v^{(i)}_1 = 0$이 된다\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Cross-Entropy Loss Function ]**\n",
    "\n",
    "- Loss Function을 Cross-Entropy Function이라고 부름\n",
    "\n",
    "- Entropy는 목적 달성을 위한 경우의 수를 정량적으로 표현하는 수치 -> **작을 수록 경우의 수가 적음**\n",
    "\n",
    "> $H(p) = -\\sum_{i=1}^n p_i log p_i$\n",
    "- $H(p)$값$\\downarrow$ $\\rightarrow$ entropy값$\\downarrow$\n",
    "- n : distint events\n",
    "- 각 event $i$는 $p_i$의 가능성을 가진다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### **Softmax with Numpy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Iris dataset ]**\n",
    "- 대표적인 Multiclass dataset. 우리가 활용할 dataset\n",
    "- 붓꽃 정보를 모아서 3가지 붓꽃 종류 중 어디에 해당하는지를 찾는 문제\n",
    "- 일반적으로 통계학자 Fisher가 제안한 데이터 셋으로 Fisher's Iris dataset이라고도 부름\n",
    "\n",
    "![img](https://ifh.cc/g/fDtTKC.png)\n",
    "![img](https://ifh.cc/g/CpGA50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "datasets = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = datasets[\"data\"]\n",
    "x_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data = datasets[\"target\"]\n",
    "y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot Encoding for Y**<br><br>\n",
    "where $v_{ij} =\n",
    "\\begin{cases}\n",
    "1\\ \\ \\  \\ \\  \\ \\;if \\ \\;y^{(i)} is \\ label \\ j\\\\\n",
    "0\\ \\ \\  \\ \\  \\ \\;if \\ \\;y^{(i)} is \\ NOT \\ label \\ j\\\\\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data = y_data.reshape([-1,1])\n",
    "y_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y_data)  \n",
    "y_data = enc.transform(y_data).toarray()\n",
    "y_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22222222, 0.625     , 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.41666667, 0.06779661, 0.04166667],\n",
       "       [0.11111111, 0.5       , 0.05084746, 0.04166667]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_data_minmax = min_max_scaler.fit_transform(x_data)\n",
    "x_data_minmax[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.22222222, 0.625     , 0.06779661, 0.04166667],\n",
       "       [1.        , 0.16666667, 0.41666667, 0.06779661, 0.04166667],\n",
       "       [1.        , 0.11111111, 0.5       , 0.05084746, 0.04166667]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_0 =np.ones(x_data_minmax.shape[0])\n",
    "x_data_minmax = np.column_stack((x_0, x_data_minmax))\n",
    "\n",
    "x_data_minmax[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0941613 , 0.51615897, 0.75572245, 0.31359467, 0.29009101],\n",
       "       [0.11561283, 0.51720294, 0.59211883, 0.61182526, 0.21761526],\n",
       "       [0.89535784, 0.59210986, 0.70364909, 0.78237412, 0.656167  ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.random.uniform(size=(3,5))\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax function**\n",
    "- $p_j^{(i)} = \\frac{e^{x^{(i)T} \\theta_j}}{\\sum_{j=1}^K e^{x^{(i)T} \\theta_j}}$\n",
    "- softmax의 인자 z $=x^T \\theta_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e = np.exp(z)\n",
    "    p = e / np.sum(np.exp(z), axis=1).reshape([-1,1])\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Entropy**\n",
    "- $-log L = -log \\prod_{i=1}^m \\prod_{j=1}^K p^{(i)^{v_{ij}}} = -\\sum_{i=1}^m \\sum_{j=1}^K v_{ij} log p^{(i)}_j$\n",
    "- 아래 코드의 y값은 [1 0 0], np.log(softmax(z))은 [0.3 0.2 0.5]이런식으로 onehot encoding된 형태로 나옴. 그 두개를 곱해줌\n",
    "- 위 식의$v_{ij}$도 onehot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_function(y, x, weights):\n",
    "    z = x_data_minmax.dot(weights.T)\n",
    "    result = - np.sum(\n",
    "                np.sum(\n",
    "                    (y * np.log(softmax(z))), axis=1).reshape((-1,1))\n",
    "                )\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174.09652003475725"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_function(y_data,x_data_minmax,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights update**\n",
    "> $w_{kj} = w_{kj} - \\alpha \\frac{\\partial l}{\\partial w_{kj}}$<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$= w_{kj} - \\alpha \\frac{\\partial l}{\\partial z_k} \\frac{\\partial z_k}{\\partial w_{kj}}$<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$= w_{kj} - \\alpha(p_k - v_k)x_j$<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$= w_{kj} + \\alpha(v_k - p_k)x_j$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ \\Rightarrow w_{kj} = w_{kj} + \\alpha \\sum_{i=1}^n (v^{(i)}_k - p^{(i)}_k)x^{(i)}_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_grdient(y, x, initial_weights, iterations = 500000, alpha=0.001):\n",
    "    cost_history= []\n",
    "    theta_history = []\n",
    "    m = y.shape[0]\n",
    "    theta = np.copy(initial_weights)\n",
    "    \n",
    "    number_of_classes = theta.shape[0]\n",
    "    number_of_weights = theta.shape[1]\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        original_theta = np.copy(theta)\n",
    "        for k in range(number_of_classes):        \n",
    "            for j in range(number_of_weights):\n",
    "                partial_x = x[:, j]\n",
    "                partial_entropy = y - softmax(x.dot(original_theta.T))\n",
    "                theta[k][j]  = original_theta[k][j] + (\n",
    "                    alpha* partial_entropy[:,k].dot(partial_x.T) ) /150\n",
    "        if (_ % 10000) == 0:\n",
    "            print(cross_entropy_function(y,x,theta)/150)\n",
    "            cost_history.append(cross_entropy_function(y,x,theta))\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1604591636178923\n",
      "0.7443923471811454\n",
      "0.6106864945702731\n",
      "0.5374371481815604\n",
      "0.48956385427280036\n",
      "0.4548017624469416\n",
      "0.4278319108681639\n",
      "0.4059568213598915\n",
      "0.38764533870895207\n",
      "0.37195318795725485\n",
      "0.3582603974942343\n",
      "0.3461397033657677\n",
      "0.3352851128590683\n",
      "0.32547054786755597\n",
      "0.31652461084819195\n",
      "0.3083145048992892\n",
      "0.30073540831389395\n",
      "0.2937032344584169\n",
      "0.287149567098\n",
      "0.28101803611910153\n",
      "0.275261672017003\n",
      "0.26984094072064024\n",
      "0.2647222608705633\n",
      "0.25987686935721804\n",
      "0.2552799422836754\n",
      "0.2509099059703648\n",
      "0.24674789120739074\n",
      "0.2427772967718355\n",
      "0.2389834372017241\n",
      "0.23535325619758163\n",
      "0.23187509161852574\n",
      "0.22853848139218666\n",
      "0.2253340021310091\n",
      "0.22225313409156305\n",
      "0.2192881475019315\n",
      "0.21643200633731888\n",
      "0.2136782864325971\n",
      "0.21102110544519356\n",
      "0.20845506266803104\n",
      "0.20597518707345575\n",
      "0.2035768922699978\n",
      "0.2012559372928615\n",
      "0.19900839234002624\n",
      "0.19683060871938565\n",
      "0.19471919239642274\n",
      "0.19267098063270996\n",
      "0.19068302128781758\n",
      "0.18875255442470593\n",
      "0.18687699591431045\n",
      "0.18505392278105942\n"
     ]
    }
   ],
   "source": [
    "# weights = minimize_grdient(y_data, x_data_minmax,weights)\n",
    "theta, cost_history = minimize_grdient(y_data, x_data_minmax,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([138,  90,  94, 126, 134,  82,   0, 144,  29,  76,  22,  90, 102,\n",
       "        43,   1,  34,  81,  11, 101,  93,  36, 147, 113,  17,  58,  54,\n",
       "        24,   8,  28,  75])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_index= np.random.randint(0,150,30)\n",
    "rand_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 2, 1, 1, 0, 2, 0, 1, 0, 1, 2, 0, 0, 0, 1, 0, 2, 1, 0, 2,\n",
       "       2, 0, 1, 1, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(softmax(x_data_minmax[rand_index].dot(theta.T)),axis=1) \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 2, 2, 1, 0, 2, 0, 1, 0, 1, 2, 0, 0, 0, 1, 0, 2, 1, 0, 2,\n",
       "       2, 0, 1, 1, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.argmax(y_data[rand_index],axis=1)\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred == y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred == y_true) / len(rand_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Metrics for Multiclass**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Contusion matrix for multiclass ]**\n",
    "- Class별로 True Positive와 Error로 분류\n",
    "- FN행 기준, FP열 기준으로 값 확인\n",
    "![img](https://ifh.cc/g/RzJJWd.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Accuracy for multiclass ]**\n",
    "- 전체 Class중 정확히 일치한 Class의 갯수\n",
    "![img](https://ifh.cc/g/HzFkFN.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Precision for multiclass ]**\n",
    "- TP / (TP + FP), 하나의 클래스와 나머지 Column 클래스\n",
    "- Precision A = TP_A / (TP_A + E_BA + E_CA + E_DA + E_EA)\n",
    "![img](https://ifh.cc/g/LcxsoM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Recall for multiclass ]**\n",
    "- TP / (TP + FN), 하나의 클래스와 나머지 Row 클래스\n",
    "- Recall A = TP_A / (TP_A + E_AB + E_AC + E_AD + E_AE)\n",
    "![img](https://ifh.cc/g/hTpvWD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### **Multiclass with sklearn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ digit dataset ]**\n",
    "- Optical Recognition of Handwritten Digits Data Set\n",
    "- 손글씨로 쓴 숫자를 분류하는 데이터셋\n",
    "- MNIST가 원조, scikt-learn에서 8by8 image 제공<br><br>\n",
    "![img](https://ifh.cc/g/BPG3Ac.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_dataset = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optical Recognition of Handwritten Digits Data Set\n",
      "===================================================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 5620\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      "References\n",
      "----------\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(digit_dataset[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- flatten하듯이 reshape해서 한 행에 64개씩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n",
       "       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n",
       "       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n",
       "       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n",
       "       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n",
       "       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n",
       "       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n",
       "       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_dataset[\"data\"][0].reshape(-1,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_dataset[\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_dataset[\"target_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [\"images\"][0]의 모습<br><br>\n",
    "![img](https://ifh.cc/g/0Jwxy6.png)\n",
    "- 8 x 8\n",
    "- 아래의 코드의 실행결과를 보면 알수 있듯이, 색이 어두우면 값이 작고 색이 밝으면 값이 커지게 표현되있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n",
       "       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n",
       "       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n",
       "       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n",
       "       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n",
       "       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n",
       "       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n",
       "       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_dataset[\"images\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(digit_dataset[\"images\"][7],cmap=plt.cm.Dark2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digit_dataset[\"data\"]\n",
    "y = digit_dataset[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,\n",
       "       2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7,\n",
       "       7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6,\n",
       "       6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4,\n",
       "       6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiclass for LogisticRegression Class**\n",
    "- multi_class : 'ovr', 'multinomial'\n",
    "\n",
    "1. ovr : One Vs Rest : 각 class마다  그 class와 나머지로 classifier만든다.\n",
    "2. multinomial : regression절차에 softmax까지 취해줘서 더 정규화된 값을 뱉는다. 아래 그림 참고<br>\n",
    "![img](https://ifh.cc/g/mtRFnP.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_ovr = LogisticRegression(multi_class=\"ovr\")\n",
    "logreg_softmax = LogisticRegression(multi_class=\"multinomial\", solver=\"sag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_ovr.fit(X_train, y_train)\n",
    "logreg_softmax.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg_ovr.predict(X_test)\n",
    "y_true = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 52,  0,  2,  0,  0,  0,  0,  2,  0],\n",
       "       [ 0,  0, 45,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 46,  0,  1,  0,  1,  0,  1],\n",
       "       [ 0,  1,  0,  0, 45,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1,  0, 45,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0, 42,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  1,  1,  0, 42,  0,  0],\n",
       "       [ 0,  3,  0,  0,  0,  0,  0,  0, 33,  2],\n",
       "       [ 0,  0,  0,  1,  0,  2,  0,  1,  1, 36]], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**classification_report**\n",
    "- 한번에 보여줌\n",
    "- print 써줘야됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        43\n",
      "          1       0.93      0.93      0.93        56\n",
      "          2       1.00      1.00      1.00        45\n",
      "          3       0.92      0.94      0.93        49\n",
      "          4       0.98      0.98      0.98        46\n",
      "          5       0.92      0.98      0.95        46\n",
      "          6       1.00      1.00      1.00        42\n",
      "          7       0.95      0.95      0.95        44\n",
      "          8       0.92      0.87      0.89        38\n",
      "          9       0.92      0.88      0.90        41\n",
      "\n",
      "avg / total       0.95      0.95      0.95       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = logreg_softmax.predict(X_test)\n",
    "y_true = y_test\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9730130576334103"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall_score(y_true, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466728518230983"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(logreg_ovr, X, y, scoring=\"accuracy\", cv=20, n_jobs=8).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9477588620029452"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logreg_softmax, X, y, scoring=\"accuracy\", cv=20, n_jobs=8).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ One vs One or One vs Rest Classifier ]**<br><br>\n",
    "**One vs One or One**\n",
    "- class 나눌 때 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OneVsRestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "y_pred = OneVsRestClassifier(logreg_ovr).fit(X_train, y_train).predict(X_test)\n",
    "y_true = y_test\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(OneVsRestClassifier(logreg_ovr).fit(X_train, y_train).estimators_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
