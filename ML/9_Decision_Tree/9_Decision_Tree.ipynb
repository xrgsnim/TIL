{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Probability Overview]** - 머신러닝의 학습방법들\n",
    "- Gradient descent based learning\n",
    "- Probability theory based learning\n",
    "- Information theory based learning - 이번에 배울거\n",
    "- Distance similarity based learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Decision Tree Classifier ]**\n",
    "- Data를 가장 잘 구분할 수 있는 Tree를 구성함\n",
    "<br><br>\n",
    "![img](https://ifh.cc/g/wrWpSY.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Decision Tree만들기 ]**\n",
    "- 어떤 질문(**Attribute**)이 가장 많은 해답(**Y**)을 줄 것인가?\n",
    "- 결국 어떤 질문이 <span style = 'background-color: #fff5b1'>답의 모호성을 줄여</span>줄 것인가?\n",
    "- 문제를 통해서 splitting point를 설정 $\\rightarrow$ 남은 정보로 spliting point를 설정하는 식\n",
    "<br><br><br>\n",
    "![img](https://ifh.cc/g/aNQFWh.png)\n",
    "- [ Decision Tree만들기 ]에 의하면 좌측의 트리가 더 답의 모호성을 줄여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### **Entrophy - Information theory**\n",
    "- 목적 달성을 위한 경우의 수를 정량적으로 표현하는 수칙 $\\rightarrow$ 작을수록 경우의 수가 적음\n",
    "- Higher Entrophy $\\rightarrow$ Higher uncertainty\n",
    "- Lower Entrophy $\\rightarrow$ Lower uncertainty\\\n",
    "$\\therefore$ **Entropy가 작으면 얻을 수 있는 정보가 많다.**\n",
    "<br><br><br>\n",
    "> $h(D) = -\\sum_{i=1}^m p_i log_2(p_i)$\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "> $ where\n",
    "> \\begin{cases}\n",
    "> D\\,\\,\\,\\,\\, Data\\, set \\\\\n",
    "> p_i \\,\\,\\,\\,\\, Probability\\,\\, of\\,\\, label\\,\\, i\n",
    "> \\end{cases}$\n",
    "<br>\n",
    "\n",
    "![img](https://ifh.cc/g/8h8RDs.png)\n",
    "<br><br>\n",
    "- $p_i$가 1에 가까워질수록 사건의 발생이 더 명확해짐을 뜻한다.\n",
    "- (특정사건이 일어날 확률 $p_i$가 1에 가까워지면 다른 사건이 일어날 확률들은 그만큼 줄어들기 때문에)\n",
    "- 따라서 $p_i$가 1에 가까워지면 entropy($h(D)$)는 0에 가까워진다.\n",
    "- 다시말해, $p_i$의 값이 다양해질수록 $p_i log_2 (p_i)$는 감소되어 entropy($h(D)$)는 증가한다.\n",
    "<br><br>\n",
    "**확률이 1이면 Entropy 0**<br>\n",
    "**확률이 작을수록 Entropy 커짐**<br>\n",
    "**Entropy는 상대적인 개념**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(예제)**<br><br>\n",
    "![img](https://ifh.cc/g/fdzSg7.png)<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('hyeooi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c36318f1f175f6468c37ab31a9400557c0f81fa065037d590afb13b0d6bda87f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
