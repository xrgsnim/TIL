{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Probability Overview]** - 머신러닝의 학습방법들\n",
    "- Gradient descent based learning\n",
    "- Probability theory based learning\n",
    "- Information theory based learning - 이번에 배울거\n",
    "- Distance similarity based learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Decision Tree Classifier ]**\n",
    "- Data를 가장 잘 구분할 수 있는 Tree를 구성함\n",
    "<br><br>\n",
    "![img](https://ifh.cc/g/wrWpSY.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Decision Tree만들기 ]**\n",
    "- 어떤 질문(**Attribute**)이 가장 많은 해답(**Y**)을 줄 것인가?\n",
    "- 결국 어떤 질문이 <span style = 'background-color: #fff5b1'>답의 모호성을 줄여</span>줄 것인가?\n",
    "- 문제를 통해서 splitting point를 설정 $\\rightarrow$ 남은 정보로 spliting point를 설정하는 식\n",
    "<br><br><br>\n",
    "![img](https://ifh.cc/g/aNQFWh.png)\n",
    "- [ Decision Tree만들기 ]에 의하면 좌측의 트리가 더 답의 모호성을 줄여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### **Entrophy - Information theory**\n",
    "- 목적 달성을 위한 경우의 수를 정량적으로 표현하는 수칙 $\\rightarrow$ 작을수록 경우의 수가 적음\n",
    "- Higher Entrophy $\\rightarrow$ Higher uncertainty\n",
    "- Lower Entrophy $\\rightarrow$ Lower uncertainty\\\n",
    "$\\therefore$ **Entropy가 작으면 얻을 수 있는 정보가 많다.**\n",
    "<br><br><br>\n",
    "> $h(D) = -\\sum_{i=1}^m p_i log_2(p_i)$\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "> $ where\n",
    "> \\begin{cases}\n",
    "> D\\,\\,\\,\\,\\, Data\\, set \\\\\n",
    "> p_i \\,\\,\\,\\,\\, Probability\\,\\, of\\,\\, label\\,\\, i\n",
    "> \\end{cases}$\n",
    "<br>\n",
    "\n",
    "![img](https://ifh.cc/g/8h8RDs.png)\n",
    "<br><br>\n",
    "- $p_i$가 1에 가까워질수록 사건의 발생이 더 명확해짐을 뜻한다.\n",
    "- (특정사건이 일어날 확률 $p_i$가 1에 가까워지면 다른 사건이 일어날 확률들은 그만큼 줄어들기 때문에)\n",
    "- 따라서 $p_i$가 1에 가까워지면 entropy($h(D)$)는 0에 가까워진다.\n",
    "- 다시말해, $p_i$의 값이 다양해질수록 $p_i log_2 (p_i)$는 감소되어 entropy($h(D)$)는 증가한다.\n",
    "<br><br>\n",
    "**확률이 1이면 Entropy 0**<br>\n",
    "**확률이 작을수록 Entropy 커짐**<br>\n",
    "**Entropy는 상대적인 개념**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(예제)**<br><br>\n",
    "![img](https://ifh.cc/g/fdzSg7.png)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Growing a Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Growing a Decision Tree ]**\n",
    "- Decision Tree를 성장(만드는) 시키는 **알고리즘**이 필요\n",
    "- 어떻게하면 가장 잘 분기(branch)를 만들수 있을까?\n",
    "- Data의 attribute를 기준으로 분기를 생성\n",
    "- 어떤 attribute를 기준으로해야 가장 entropy가 작은가?\n",
    "- 하나를 자른 후에 그 다음은 어떻게 할 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**예제**<br>\n",
    "\n",
    "- 컴퓨터를 사는지 여부를 확인하는 데이터<br><br>\n",
    "![img](https://ifh.cc/g/LybK23.png)\n",
    "<br><br>\n",
    "![img](https://ifh.cc/g/36KsQL.png)\n",
    "1. 가장 entropy가 작게끔 Age - Middle_aged로 분기\n",
    "2. 남은 데이터 상에서 entorpy가 작게 재귀적으로 분기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision Tree는 **재귀적**으로 생김\n",
    "- 대상 라벨에 대해 어떤 **Attribute**가 더 **확실한 정보**를 제공하고 있는가?로 branch attribute(Entropy, Gini 등)를 선택\n",
    "- 확실한 정보의 선택 기준은 알고리즘별로 차이가 남\n",
    "- Tree생성 후 prunning을 통해 Tree generalization 시행\n",
    "- 일반적으로 효율을 위해 Binary Tree를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Decision Tree의 특징 ]**\n",
    "- 비교적 간단하고 직관적으로 결과를 표현\n",
    "- 훈련시간이 길고, 메모리 공간을 많이 사용함\n",
    "- Top-down, Recursive, Divide and Conquer기법\n",
    "- Greedy 알고리즘 -> 부분 최적화 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Decision Tre의 장점 ]**\n",
    "- 트리의 **상단** 부분 attribute들이 가장 중요한 예측변수 $\\rightarrow$ attribute 선택 기법으로도 활용할 수 있음\n",
    "- Attribute의 scailing이 필요없음\n",
    "- 관측치의 절대값이 아닌 **순서**가 중요 $\\rightarrow$ Outlier에 이점\n",
    "- 자동적 변수 부분선택 $\\leftarrow$ Tree prunning\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Algorithms of Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **ID3 & Information Gain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Information Gain ]**\n",
    "- Entropy함수를 도입하여 branch splitting\n",
    "- Information Gain : Entropy를 사용하여 속성별 분류시 **Impurity를 측정**하는 지표\n",
    "- (전체 Entropy - 속성별 Entropy)로 속성별 Information Gain을 계산\n",
    "- **속성별 Entropy가 낮은 attribute로 분기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전체 데이터 D의 정보량\n",
    "> $Info(D) = - \\sum_{i=1}^n p_i log_2 (p_i)$\n",
    "- 속성 A로 분류시 정보량\n",
    "> $Info_A (D) = -\\sum_{j=1}^v \\frac{|D_j|}{|D|} * Info(D_j)$\n",
    "- A 속성의 정보 소득\n",
    "> $Gain(A) = Info(D) - Info_A (D)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('hyeooi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c36318f1f175f6468c37ab31a9400557c0f81fa065037d590afb13b0d6bda87f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
