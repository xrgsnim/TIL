{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mathematics for Artificial Intelligence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Vector**\n",
    "- 숫자를 원소로 가지는 리스트(list)또는 배열(array)\n",
    "- 공간 상의 한 점\n",
    "\n",
    "<br><br>\n",
    "**[ norm(노름) ]** : 원점에서부터의 거리\n",
    "\n",
    "$x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix}$\n",
    "<br><br>\n",
    "$L_1 : ||x||_1 = \\sum_{i=1}^d |x_i|$ : 각 성분의 변화량의 절대값을 모두 더한다.\n",
    "<br><br>\n",
    "$L_2 : ||x||_2 = \\sqrt{\\sum_{i=1}^d |x_i|^2}$ : 피타고라스 정리를 이용해 유클리드 거리를 계산한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_norm(x):\n",
    "    x_norm = np.abs(x)\n",
    "    x_norm = np.sum(x_norm)\n",
    "    return x_norm\n",
    "\n",
    "def l2_norm(x):\n",
    "    x_norm = x*x\n",
    "    x_norm = np.sum(x_norm)\n",
    "    x_norm = np.sqrt(x_norm)\n",
    "    return x_norm\n",
    "\n",
    "# np.linalg.norm 으로도 구현 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 노름의 종류에 따라 **기하학적 성질**이 달라진다.\n",
    "- 머신러닝에선 각 노름의 성질에 따라 다른 상황에 쓰인다.\n",
    "<br><br>\n",
    "![Image](https://ifh.cc/g/dFWFSS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 두 벡터 사이의 거리 ]**\n",
    "<br>\n",
    "- $L_1, L_2$노름을 이용해 **두 벡터 사이의 거리**를 계산\n",
    "- 두 벡터 사이의 거리를 계산할 때는 **벡터의 뺄셈**을 이용\n",
    "- 뺄셈을 거꾸로 해도 거리는 같다.\n",
    "<br><br>\n",
    "![Image](https://ifh.cc/g/HsHbt5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 두 벡터 사이의 각도 ]**\n",
    "- 두 벡터 사이의 거리를 이용하여 각도를 계산\n",
    "- 2,3,..,d차원 등 어떤 차원에서도 각도 계산 가능\n",
    "- **제2 코사인 법칙**에 의해 두 벡터 사이의 각도를 계산\n",
    "- 분자를 쉽게 계산하는 방법은 **$L_2$노름** 과 **내적**\n",
    "<br><br>\n",
    "![Image](https://ifh.cc/g/sspAkR.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 꼭 2,3차원이 아닌 임의의 d차원에서도 계산 가능\n",
    "def angle(x, y):\n",
    "    v = np.inner(x, y) / (l2_norm(x) * l2_norm(y)) # inner : 내적\n",
    "    theta = np.arccos(v)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 내적 ]**\n",
    "<br>\n",
    "- 내적 : **정사영(orthogonal projection)된 벡터의 길이**와 관련\n",
    "- Proj(x)의 길이는 **코사인법칙**에 의해 $||x||cos \\theta$\n",
    "- 내적은 정사영의 길이를 **벡터y의 길이 ||y||만큼 조정**한 값이다.\n",
    "<br><br>\n",
    "![Image](https://ifh.cc/g/1vXQT5.png)\n",
    "<br><br>\n",
    "- 내적은 두 벡터, 즉 두 데이터 간의 유사도(Similarity)를 측정하는데 사용가능하다.\n",
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Matrix**\n",
    "- **벡터**를 원소로 가지는 2차원 배열\n",
    "- 공간 상의 여러 점\n",
    "- **!! numpy에서는 행벡터가 기본 단위 !!**\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ Transpose(전치행렬) ]**\n",
    "<br><br>\n",
    "![Image](https://ifh.cc/g/mt9coT.png)\n",
    "- 행과 열의 인덱스가 바뀐 행렬\n",
    "- $X^T = (X_{ij}) \\Longrightarrow (X_{ji})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 행렬의 이해 - 1 ]**\n",
    "- 벡터가 공간에서 한 점을 의미한다면 행렬은 여러 점들을 의미 \n",
    "- 행렬의 행벡터 **$\\mathbf{x_i}$는 i번째 데이터**를 의미\n",
    "- 행렬의 **$x_{ij}$는 i번째 데이터의 j번째 변수**의 값\n",
    "![Image](https://ifh.cc/g/qo2NZ5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 행렬 곱셈 ]**\n",
    "- 행렬 곱셈(matrix multiplication)은 i번째 행벡터와 j번째 열벡터 사이의 내적을 성분으로 가지는 행렬을 계산\n",
    "- $XY = (\\sum_k x_{ik} y_{kj})$\n",
    "- numpy에서는 @연산을 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 22],\n",
       "       [43, 50]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,2],\n",
    "             [3,4]])\n",
    "\n",
    "Y = np.array([[5,6],\n",
    "             [7,8]])\n",
    "\n",
    "X @ Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 행렬 내적 ]**\n",
    "- numpy의 <span style='background-color: #fff5b1'><span style=\"color:red\">np.inner</span></span>는 **i번째 행벡터와 j번째 행벡터 사이의 내적**을 성분으로 가지는 행렬을 계산\n",
    "- **수학에서의 내적과 다르므로 주의 !!!**\n",
    "- 따라서 두 행렬의 행벡터의 크기가 같아야 연산가능\n",
    "\n",
    "![Image](https://ifh.cc/g/4pqBHj.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5,  3],\n",
       "       [ 5,  2],\n",
       "       [-3, -1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1, -2, 3],\n",
    "              [7, 5, 0],\n",
    "              [-2, -1, 2]])\n",
    "\n",
    "Y = np.array([[0, 1, -1],\n",
    "              [1, -1, 0]])\n",
    "\n",
    "np.inner(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 행렬의 이해 - 2 ]**\n",
    "\n",
    "- 행렬은 **벡터공간에서 사용되는**, **두 데이터를 연결해주는 연산자**(operator)로 이해한다.\n",
    "- 행렬곱을 통해 벡터를 **다른 차원의 공간**으로 보낼 수 있다.\n",
    "- 행렬곱을 통해 **패턴을 추출**할 수 있고 **데이터를 압축**할 수도 있다.\n",
    "- 모든 선형변환(linear transform)은 행렬곱으로 계산할 수 있다.\n",
    "\n",
    "![Image](https://ifh.cc/g/Lx9a3T.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 역행렬의 이해 ]**\n",
    "- 역행렬(Inverse matrix) : 어떤 행렬 $A$의 연산을 거꾸로 되돌리는 행렬, $A^{-1}$로 표기\n",
    "- 역행렬이 존재하기위한 조건\n",
    "   1. 행,열 의 크기가 같다.\n",
    "   2. $det(A)\\ne 0$\n",
    "- 연산자 A가 벡터를 다른 차원의 공간으로 보냈다면 역행렬을 이용해 다시 원래 차원의 공간으로 데려온다.\n",
    "   \n",
    "![Image](https://ifh.cc/g/7h2Q0M.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, -2, 3],\n",
    "              [7, 5, 0],\n",
    "              [-2, -1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21276596,  0.0212766 , -0.31914894],\n",
       "       [-0.29787234,  0.17021277,  0.44680851],\n",
       "       [ 0.06382979,  0.10638298,  0.40425532]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00, -1.38777878e-17,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00],\n",
       "       [-2.77555756e-17,  0.00000000e+00,  1.00000000e+00]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X @ np.linalg.inv(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 유사역행렬(pseudo-inverse) or 무어-펜로즈(Moore-Penrose)역행렬 ]**\n",
    "- 행, 열의 크기가 달라서 역행렬을 계산 못할 때 이용. $A^+$로 표기\n",
    "\n",
    "$n \\ge m$인 경우 : $A^+ = (A^T A)^{-1} A^T$\n",
    "<br>\n",
    "$n \\le m$인 경우 : $A^+ = A^T (A A^T)^{-1} $\n",
    "\n",
    "- !! !$n \\ge m$ 이면 $ A^+ A = I$가 성립!!!\n",
    "- !!! $n \\le m$ 이면 $ A A^+ = I$가 성립!!!\n",
    "- 유사역행렬은 연산순서가 달라지면 결과도 바뀌니까 연산순서에 주의\n",
    "\n",
    "![Image](https://ifh.cc/g/7h2Q0M.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([[0, 1],\n",
    "              [1, -1],\n",
    "              [-2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.00000000e-01,  1.11022302e-16, -5.00000000e-01],\n",
       "       [ 8.33333333e-01, -3.33333333e-01, -1.66666667e-01]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 유사역행렬(pseudo-inverse)의 p를 따와서 pinv\n",
    "np.linalg.pinv(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00, -2.22044605e-16],\n",
       "       [ 1.11022302e-16,  1.00000000e+00]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(Y) @ Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 역행렬의 연산 예제 : 선형회귀분석]**\n",
    "- np.linalg.pinv를 이용하면 데이터를 선형모델(linear model)로 해석하는 선형회귀식을 찾을수 있다.\n",
    "\n",
    "![Image](https://ifh.cc/g/SAsqB0.png)\n",
    "- $n \\ge m$인 경우 : 데이터의 변수 개수보다 많거나 같아야 함\n",
    "- Moore-Penrose 역행렬을 이용하면 $y$에 근접하는 $\\hat{y}$를 찾을 수 있다.\n",
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **경사하강법 (순한맛)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 미분(differentiation) ]**\n",
    "- **변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구**로 최적화에서 제일 많이 사용하는 기법\n",
    "> $f \\prime (x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}$\n",
    "\n",
    "- 예제) $f(x) = x^2 + 2x + 3$\n",
    "<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $f \\prime (x) = 2x + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\operatorname{Poly}{\\left( 2 x + 2, x, domain=\\mathbb{Z} \\right)}$"
      ],
      "text/plain": [
       "Poly(2*x + 2, x, domain='ZZ')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sym\n",
    "from sympy.abc import x\n",
    "\n",
    "# diff(a,b) : 미분, a를 b로 미분하라\n",
    "# poly() : 괄호 안을 다항함수라고 지칭\n",
    "sym.diff(sym.poly(x**2 + 2*x + 3), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그림으로 이해 : 함수 $f$의 주어진 점 $(x, f(x))$에서의 **접선의 기울기**를 구한다.\n",
    "- 한 점에서 접선의 기울기를 알면 어느 방향으로 점을 움직여야 함수값이 **증가/감소**하는지 알 수 있다.\n",
    "\n",
    "![Image](https://ifh.cc/g/YRrF7S.png)\n",
    "- 아래 함수에서 $h$를 0으로 보내면 $(x, f(x))$에서 접선의 기울기로 수렴한다.\n",
    "- **증가시키고 싶다면 미분값을 더하고, 감소시키고 싶다면 미분값을 뺀다.**\n",
    "- **즉, 차수가 높은 함수(데이터)를 다룰 때 어디로 이동해서 값이 증가,감소하는지 모르니 이 때 해당 함수를 미분하여 더하거나 빼준다.(미분값이 양수든 음수든 더해주면 함수값이 증가, 빼주면 감소한다.)**\n",
    "\n",
    "![Image](https://ifh.cc/g/LmRPGd.png)\n",
    "- **미분값을 더하면 경사상승법(gradient ascent)** 이라 하며 함수의 **극대값**의 위치를 구할 때 사용한다.\n",
    "- **미분값을 뻬면 경사하강법(gradient descent)** 이라 하며 함수의 **극소값**의 위치를 구할 때 사용한다.\n",
    "- **경사상승 / 경사하강** 방법은 극값에 도달하면 움직임이 멈춘다\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 경사하강법 : 알고리즘 ]**\n",
    "- Input : gradient(미분을 계산하는 함수), init(시작점), lr(학습률), eps(알고리즘 종료조건)\n",
    "- Output : var\n",
    "<br><br>\n",
    "> var = init <br>\n",
    "grad = gradient(var) <br>\n",
    "while(abs(grad) > eps) : <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; var = var - lr* grad <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; grad = gradient(var)\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 변수가 벡터일 때 ]**\n",
    "- 벡터가 입력인 다변수 함수의 경우 편미분(partial differentiation)을 사용\n",
    "- 예) x에 대해서만 미분 -> y는 상수 취급\n",
    "> #### $ \\partial_{xi} f(x) - \\lim_{h \\to 0} \\frac{f(x + he_i - f(x))}{h}$\n",
    "> 예) $f(x, y) = x^2 + 2xy + 3 + cos(x + 2y)$ <br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; $\\partial_x f(x,y) = 2x + 2y - sin(x + 2y) $ : x방향으로의 편미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 변수 별로 편미분을 계산한 그레디언트(gradient) 벡터를 이용하여 경사하강 / 경사상승법에 사용할 수 있다.\n",
    "> $\\nabla f = (\\partial_{x1}f, \\partial_{x2}f, \\cdots, \\partial_{xd}f)$ <br>\n",
    "> 미분값을 나타내는 $f\\prime (x)$ 대신 벡터 $\\nabla f$를 사용하여 변수 $x = (x_1, \\cdots, x_d)$를 동시에 업데이트 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 그레디언트 벡터 ]**\n",
    "\n",
    "![Image](https://ifh.cc/g/XxDy3b.png)\n",
    "- 각 점 (x, y, z)공간에서 f(x,y)표면을 따라 $- \\nabla f$벡터를 그리면 위와 같이 그려진다\n",
    "\n",
    "> $f(x, y) = x^2 + 2y^2$ <br><br>\n",
    "> $\\Rightarrow \\nabla f = -(2x, 4y)$\n",
    "- $- \\nabla f$는 $ \\nabla (-f)$랑 같고 이는 각 점에서 **가장 빨리 감소하게 되는 방향**과 같다.\n",
    "<br>\n",
    "\n",
    "![Image](https://ifh.cc/g/1hMzh5.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 변수가 벡터일 때의 경사하강법 알고리즘 ]**\n",
    "- Input : gradient(**그레디언트 벡터를 계산하는 함수**), init(시작점), lr(학습률), eps(알고리즘 종료조건)\n",
    "- Output : var\n",
    "<br><br>\n",
    "> var = init <br>\n",
    "grad = gradient(var) <br>\n",
    "while(**norm**(grad) > eps) : <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; var = var - lr* grad <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; grad = gradient(var)\n",
    "<br><br>\n",
    "\n",
    "- 경사하강법의 알고리즘은 그대로 적용된다.\n",
    "- 그러나 벡터는 절대값 **노름(norm)** 을 계산해서 종료조건을 설정한다.\n",
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **경사하강법(매운맛)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "### **확률론**\n",
    "- 딥러닝은 **확률론 기반의 기계학습 이론**에 바탕\n",
    "- 기계학습에서 사용되는 손실함수(Loss function)들의 작동원리는 데이터 공간을 통계적으로 해석해서 유도\n",
    "- 회귀 분석에서 손실함수로 사용되는 $L_2-$노름은 **예측오차의 분산을 가장 최소화하는 방향으로 학습**하도록 유도\n",
    "- 분류 문제에서 사용되는 교차엔트로피(cross_entropy)는 **모델 예측의 불확실성을 최소화하는 방향으로 학습**하도록 유도\n",
    "- **분산 및 불확실성을 최소화**하기 위해서는 측정하는 방법을 알아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 확률분포 : 데이터의 초상화 ]**\n",
    "- 데이터 공간을 $X \\times Y$라 표기하고 $D$는 데이터공간에서 데이터를 추출하는 분포\n",
    "- 데이터는 확률변수 $(x,y)$ ~ $D$라 표기\n",
    "- 결합분포 $P{(x,y)}$는 $D$를 모델링\n",
    "- $P{(x)}$는 입력 x에 대한 주변확률분포로 y에 대한 정보를 주진 않는다.\n",
    "> $P(x) = \\sum_y P(x,y)$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $P(x) = \\int_y P(x,y)dy$\n",
    "- 조건부확률분포 : 데이터 공간에서 입력x와 출력y사이의 관계를 모델링, $P(x|y)$\n",
    "\n",
    "![Image](https://ifh.cc/g/mRXKOG.png) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "![Image](https://ifh.cc/g/M9BJlB.png)\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;주변확률분포 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 조건부확률분포"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 이산확률변수 vs 연속확률변수 ]**\n",
    "- 확률변수는 확률분포 $D$에 따라 이산형(discrete)과 연속형(continuous)확률변수로 구분\n",
    "- 이산형 확률변수 : 확률변수가 가징 수 있는 모든 경우의 수를 고려하여 확률들 더해 모델링\n",
    "> $\\mathbb{P}{(X \\in A)} = \\sum_{x \\in A} P(X = x)$\n",
    "- 연속형 확률변수 : 데이터 공간에 정의된 확률변수의 밀도(density)위에서의 적분을 통해 모델링\n",
    "> $\\mathbb{P}{(X \\in A)} = \\int_{A} P(x)dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 조건부확률과 기계학습 ]**\n",
    "- $P(y|x)$ : 입력변수 x에 대해 정답이 y일 확률\n",
    "    - (단, 연속확률분포의 경우 $P(y|x)$는 확률이 아니고 **밀도**)\n",
    "- 로지스틱 회귀에서 사용했던 선형모델과 소프트맥스 함수의 결합은 **데이터에서 추출된 패턴을 기반으로 확률을 해석**하는데 사용된다.\n",
    "- 분류 문제에서 softmax $(W \\phi + b)$은 데이터 $x$로부터 추출된 특징패턴 $\\phi(x)$과 가중치행렬 W을 통해 조건부 확률 $P(y|x)$을 계산\n",
    "- 회귀 문제의 경우 조건부 기대값 $\\mathbb{E}[y|x]$을 추정\n",
    "    - $\\mathbb{E}_{y~P(y|x)} [y|x] = \\int_y yP(y|x)dy$\n",
    "- 딥러닝은 다층신경망을 사용하여 데이터로부터 특정패턴 $\\phi$를 추출\n",
    "    - 특정 패턴을 학습하기 위해 어떤 손실함수를 사용할지는 기계학습 문제와 모델에 의해 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 몬테카를로 샘플링 ]**\n",
    "- 확률 분포를 모를때 (기계학습의 많은 문제들은 확률분포를 명시적으로 모름)\n",
    "- 데이터를 이용하여 기대값을 계산하려면 몬테카를로(Monte Carlo)샘플링 방법을 사용\n",
    "> $\\mathbb{E_{x \\sim P(x)}}[f(x)]\\approx \\frac{1}{N} \\sum_{i=1}^N f(x^{(i)})$,\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "> $x^{(i)} \\sim P(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "263930470851f494f0ed2879c35b57985588df20f9e529b86e97dd5eb9ddc466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
